{"meta":{"version":1,"warehouse":"2.2.0"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":0,"renderable":0},{"_id":"source/favicon.ico","path":"favicon.ico","modified":0,"renderable":0},{"_id":"source/robots.txt","path":"robots.txt","modified":0,"renderable":0},{"_id":"source/img/icon.png","path":"img/icon.png","modified":0,"renderable":0},{"_id":"themes/light/source/favicon.ico","path":"favicon.ico","modified":0,"renderable":1},{"_id":"themes/light/source/css/style.styl","path":"css/style.styl","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/blank.gif","path":"fancybox/blank.gif","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","path":"fancybox/fancybox_loading.gif","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","path":"fancybox/fancybox_overlay.png","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","path":"fancybox/fancybox_loading@2x.gif","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","path":"fancybox/fancybox_sprite.png","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","path":"fancybox/fancybox_sprite@2x.png","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","path":"fancybox/jquery.fancybox.css","modified":0,"renderable":1},{"_id":"themes/light/source/js/gallery.js","path":"js/gallery.js","modified":0,"renderable":1},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","path":"fancybox/jquery.fancybox.pack.js","modified":0,"renderable":1},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","path":"js/jquery.imagesloaded.min.js","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","path":"css/font/fontawesome-webfont.eot","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","path":"css/font/fontawesome-webfont.woff","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","path":"css/font/fontawesome-webfont.ttf","modified":0,"renderable":1},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","path":"css/font/fontawesome-webfont.svg","modified":0,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"a2efcd24f9642d8162280a896e8573822ef6cabc","modified":1505107128000},{"_id":"source/CNAME","hash":"ae798f220c3c3a5a961c55be028adc35ba7d53e7","modified":1450694698000},{"_id":"source/favicon.ico","hash":"a3affd9aa40692b5d9e617e37d407af82716d948","modified":1452428851000},{"_id":"source/index.html","hash":"fdbed55a20e2ae6496b217d2592c9973dd706fb6","modified":1452480764000},{"_id":"source/robots.txt","hash":"05d15fa9cdd47d3f5ff815e474a8cc74e33234a9","modified":1504257890000},{"_id":"themes/light/.DS_Store","hash":"3fbb93150ac81cfce987a550f5cf064cdd880c82","modified":1504682243000},{"_id":"themes/light/.gitignore","hash":"eaa3d84cb77d92a21b111fd1e37f53edc1ff9de0","modified":1450703430000},{"_id":"themes/light/_config.yml","hash":"627ec77dab793ff72c1f35f47ad6eb8da3a3e622","modified":1452509626000},{"_id":"themes/light/LICENSE","hash":"17d3ed51d6d6962155f7dacd4f101f4969ad0bbe","modified":1450703430000},{"_id":"themes/light/README.md","hash":"35ece079c72f3ef51704f113cb34317228c7e095","modified":1452428366000},{"_id":"source/_posts/.DS_Store","hash":"9b46e83e3033f96c593d0a94c211e8a8432ca258","modified":1504008954000},{"_id":"source/_posts/DDNS.md","hash":"604246cde8d3bfe8795bcffaa4310a8c584ada2d","modified":1504009817000},{"_id":"source/_posts/SVO-SEMI-DIRECT-MONOCULAR-VISUAL-ODOMETRY-详解.md","hash":"05355176fc6f0eeb26aacd50967934cfffe08c91","modified":1505107292000},{"_id":"source/_posts/Principle-of-real-time-video-transmission.md","hash":"d6a1e03f3a18c1013179d0c7fa84323cb1b8b733","modified":1452426357000},{"_id":"source/_posts/What-is-robot.md","hash":"9863c6b4d934407400b83b6d820b15e106c1719f","modified":1452508168000},{"_id":"source/_posts/land-robot-already-open.md","hash":"52c61bcf72487c38f73fbdb74ebee514a184521c","modified":1452426544000},{"_id":"source/_posts/robot-control-principle.md","hash":"275ab224ff19bc2174c89075df862eeeb0a56f69","modified":1452479393000},{"_id":"source/_posts/the-big-data-is-coming.md","hash":"77760faf14f7259c59ddb4cf8ecc4bb38a9e4791","modified":1452507982000},{"_id":"source/_posts/this-is-a-first-blog.md","hash":"aac4572b84edc7efe0e86b6bb4d6ee81ebfa62ec","modified":1452426309000},{"_id":"source/Blog/index.md","hash":"81fb524a64375b9092e5e9cb84cff9dc941e4932","modified":1503715858000},{"_id":"source/about/index-chinese.md","hash":"ae6ea0055d9e2719b8b567d36f16e07520e5397c","modified":1460371255000},{"_id":"source/about/index.md","hash":"7d19dca1581ff2ac8dc6b249883215f73474611e","modified":1452776859000},{"_id":"source/new-page/index.md","hash":"d81045b753dad528050c658c75b413a0c2aee823","modified":1452504004000},{"_id":"source/img/icon.png","hash":"d31a0b387102f1a8d0cfec93eed63aa99847ffc3","modified":1422771435000},{"_id":"themes/light/.git/FETCH_HEAD","hash":"b625e546cff505cdd008b5c7011864b2629c242a","modified":1450703468000},{"_id":"themes/light/.git/HEAD","hash":"acbaef275e46a7f14c1ef456fff2c8bbe8c84724","modified":1450703430000},{"_id":"themes/light/.git/ORIG_HEAD","hash":"41e9d2be2a83d6c9c1b4ef28212da4f16c7f7d79","modified":1450703468000},{"_id":"themes/light/.git/description","hash":"9635f1b7e12c045212819dd934d809ef07efa2f4","modified":1450703426000},{"_id":"themes/light/.git/config","hash":"48b90552e31562810944bee8921d63ecd54a6cc2","modified":1450703430000},{"_id":"themes/light/.git/index","hash":"dcfeeb9fa7e63a8e78b2c1f4ea25fc81d3585e1b","modified":1450703431000},{"_id":"themes/light/.git/packed-refs","hash":"d044786b837a1a02dbb66de46688d9b120608076","modified":1450703430000},{"_id":"themes/light/languages/de.yml","hash":"1ebe2d4f1b48c84e004c933bec65731fb54c9998","modified":1450703430000},{"_id":"themes/light/languages/es.yml","hash":"727707b95580bbe9773edef4c84a9735fd537742","modified":1450703430000},{"_id":"themes/light/languages/default.yml","hash":"feb1dd022dc8897d65baa5b927de2a3d4178d798","modified":1450703430000},{"_id":"themes/light/languages/pl.yml","hash":"19313b4c80c50196c364afe932dad3a975782817","modified":1450703430000},{"_id":"themes/light/languages/zh-CN.yml","hash":"751f1a2bbfb6736c71e846493ae0fb7ef7dc778d","modified":1450703430000},{"_id":"themes/light/languages/ru.yml","hash":"37161bb9b6cc2dae1f53837185be32e7a0b8abfa","modified":1450703430000},{"_id":"themes/light/languages/zh-TW.yml","hash":"b2474b775a8fa0fa3e9e3c58ddb11b20cf65dbc5","modified":1450703430000},{"_id":"themes/light/layout/.DS_Store","hash":"0bee22f85ea706e34182542bea742ec4b2928115","modified":1504682243000},{"_id":"themes/light/layout/archive.ejs","hash":"a18842e3d719fe3ca9b977a6995f8facc75c8673","modified":1450703430000},{"_id":"themes/light/layout/layout.ejs","hash":"6999916072898aedfe13f4a07211dd1578ad4799","modified":1450703430000},{"_id":"themes/light/layout/category.ejs","hash":"9b740fc33f6f028df60f0bc4312bf3ebd03aa8ea","modified":1450703430000},{"_id":"themes/light/layout/index.ejs","hash":"c7cf84c84c26f1adfc249bc9a7605206fa245f73","modified":1450703430000},{"_id":"themes/light/layout/page.ejs","hash":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1450703430000},{"_id":"themes/light/layout/post.ejs","hash":"70cbc9854655773cc6ba84eecaaf330fed430465","modified":1450703430000},{"_id":"themes/light/layout/tag.ejs","hash":"45150a2365768b6b67880193c9264ad2bb4814db","modified":1450703430000},{"_id":"themes/light/source/favicon.ico","hash":"a3affd9aa40692b5d9e617e37d407af82716d948","modified":1452428851000},{"_id":"themes/light/.git/hooks/commit-msg.sample","hash":"ee1ed5aad98a435f2020b6de35c173b75d9affac","modified":1450703426000},{"_id":"themes/light/.git/hooks/applypatch-msg.sample","hash":"4de88eb95a5e93fd27e78b5fb3b5231a8d8917dd","modified":1450703426000},{"_id":"themes/light/.git/hooks/post-update.sample","hash":"b614c2f63da7dca9f1db2e7ade61ef30448fc96c","modified":1450703426000},{"_id":"themes/light/.git/hooks/pre-applypatch.sample","hash":"f208287c1a92525de9f5462e905a9d31de1e2d75","modified":1450703426000},{"_id":"themes/light/.git/hooks/pre-push.sample","hash":"5c8518bfd1d1d3d2c1a7194994c0a16d8a313a41","modified":1450703426000},{"_id":"themes/light/.git/hooks/pre-commit.sample","hash":"36aed8976dcc08b5076844f0ec645b18bc37758f","modified":1450703426000},{"_id":"themes/light/.git/hooks/pre-rebase.sample","hash":"5885a56ab4fca8075a05a562d005e922cde9853b","modified":1450703426000},{"_id":"themes/light/.git/hooks/prepare-commit-msg.sample","hash":"2b6275eda365cad50d167fe3a387c9bc9fedd54f","modified":1450703426000},{"_id":"themes/light/.git/hooks/update.sample","hash":"39355a075977d05708ef74e1b66d09a36e486df1","modified":1450703426000},{"_id":"themes/light/.git/info/exclude","hash":"c879df015d97615050afa7b9641e3352a1e701ac","modified":1450703426000},{"_id":"themes/light/.git/logs/HEAD","hash":"40b45082115e59ecfcf9d18c8e3e2ad5cfb3c606","modified":1450703430000},{"_id":"themes/light/layout/_partial/after_footer.ejs","hash":"ef5e7c5e3a05c80f25a7173ea742646bb521cfed","modified":1450703430000},{"_id":"themes/light/layout/_partial/archive.ejs","hash":"eaab5ad657f16dfc6cff6f462e1234c3cb8f23a0","modified":1450703430000},{"_id":"themes/light/layout/_partial/article.ejs","hash":"f54a5deea365915baf4a99bbdbd9053d19833a56","modified":1452424616000},{"_id":"themes/light/layout/_partial/baidu_tongji.ejs","hash":"068b395cfe7ceec1b12eea71213f771e97d2f9ed","modified":1452433252000},{"_id":"themes/light/layout/_partial/facebook_comment.ejs","hash":"49ee54e84fe2b70bd9540e2eeba5a85f744941b0","modified":1450703430000},{"_id":"themes/light/layout/_partial/comment.ejs","hash":"d96e2dd982a52bea6735dba6175c49ce06dc9bf3","modified":1452432406000},{"_id":"themes/light/layout/_partial/footer.ejs","hash":"092bea0a0cdb4ab976723bbce3771bc35a96fdc6","modified":1450703430000},{"_id":"themes/light/layout/_partial/google_analytics.ejs","hash":"d70d287956e90e99ba35b2e14cefb477f9203aa0","modified":1450703430000},{"_id":"themes/light/layout/_partial/head.ejs","hash":"da5ab9c4c3e15a832d39eb2751c9c70c10d4cb76","modified":1452433271000},{"_id":"themes/light/layout/_partial/header.ejs","hash":"c750b9f3eb3f0c50d6bb7c8568e549a2ccdf8949","modified":1452487545000},{"_id":"themes/light/layout/_partial/pagination.ejs","hash":"5a3c65842354b04c7839c915c045d39be9342cdf","modified":1450703430000},{"_id":"themes/light/layout/_partial/sidebar.ejs","hash":"016441ca9534769d8e151cffe4027686e9c86f18","modified":1450703430000},{"_id":"themes/light/layout/_widget/blogroll.ejs","hash":"a0e8f315f57f7a1f98a6fcec830ace20d1d05fef","modified":1452512443000},{"_id":"themes/light/layout/_widget/category.ejs","hash":"c163a146b0f963f257ddcc244f413bef281fe0a0","modified":1450703430000},{"_id":"themes/light/layout/_widget/recent_posts.ejs","hash":"59f6f8362fa23a6215e3381151a59c2e2a5fd0d3","modified":1450703430000},{"_id":"themes/light/layout/_widget/search.ejs","hash":"93d4a690494dfa405024f23511846ea00d647be7","modified":1450703430000},{"_id":"themes/light/layout/_widget/tag.ejs","hash":"6bf8214fedb8d6306e017e07ad67aab956496500","modified":1450703430000},{"_id":"themes/light/layout/_widget/tagcloud.ejs","hash":"139e91b1e6abcc1e3883bcc03a9a1a7f1d891d7a","modified":1450703430000},{"_id":"themes/light/layout/_widget/weibo.ejs","hash":"a08c74bbe9b8e394ad1960b2e440a0f8c37fa652","modified":1452480638000},{"_id":"themes/light/source/css/style.styl","hash":"8e8458e78717c49c4ff278b741258d77301f6be4","modified":1450703431000},{"_id":"themes/light/source/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1450703431000},{"_id":"themes/light/source/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1450703431000},{"_id":"themes/light/source/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1450703431000},{"_id":"themes/light/source/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1450703431000},{"_id":"themes/light/source/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1450703431000},{"_id":"themes/light/source/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1450703431000},{"_id":"themes/light/source/fancybox/jquery.fancybox.css","hash":"82f33ad0842aa9c154d029e0dada2497d4eb1d57","modified":1450703431000},{"_id":"themes/light/source/js/gallery.js","hash":"735a714e54f0ac229f292a90df3a1f882904f6c7","modified":1450703431000},{"_id":"themes/light/source/fancybox/jquery.fancybox.pack.js","hash":"ae6318aeb62ad4ce7a7e9a4cdacd93ffb004f0fb","modified":1450703431000},{"_id":"themes/light/source/js/jquery.imagesloaded.min.js","hash":"28ef4346743a60c896a9ae492a544c0854904350","modified":1450703431000},{"_id":"themes/light/source/css/_base/utils.styl","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1450703430000},{"_id":"themes/light/.git/refs/heads/master","hash":"41e9d2be2a83d6c9c1b4ef28212da4f16c7f7d79","modified":1450703430000},{"_id":"themes/light/.git/objects/pack/pack-1d269b2ca4df8c539fe0c39db96c64da0eb7dca1.idx","hash":"21b41520fb3f0ae759f1db92396b5505f83b1bc6","modified":1450703430000},{"_id":"themes/light/layout/_partial/post/gallery.ejs","hash":"fc23ef9b5a412e05436f68ff47146b860d2d4225","modified":1450703430000},{"_id":"themes/light/layout/_partial/post/category.ejs","hash":"8bb3f6ee6296df5a0d527b30d5a46a2387b97cb7","modified":1450703430000},{"_id":"themes/light/layout/_partial/post/share.ejs","hash":"991cf130c37f08c7e948772fb45587592b165b55","modified":1450703430000},{"_id":"themes/light/layout/_partial/post/title.ejs","hash":"7f93b310927d6238effdde15234d8cb242940893","modified":1450703430000},{"_id":"themes/light/layout/_partial/post/tag.ejs","hash":"b21bbfb5479bd5968a610ba8bdb2bdf10d7a40e9","modified":1450703430000},{"_id":"themes/light/source/css/_base/layout.styl","hash":"b26cd4768466f637b620029b314dd50a06c98b8c","modified":1450703430000},{"_id":"themes/light/source/css/_base/variable.styl","hash":"832fecbb623c8fba9419d6a93586126d44b43cc6","modified":1450703430000},{"_id":"themes/light/source/css/_partial/archive.styl","hash":"b6fa84ea80bfbdb3a93f64c06a8c652e4242128e","modified":1450703430000},{"_id":"themes/light/source/css/_partial/article.styl","hash":"9efddb26851b45a536898a84f6fa1ccf71920fca","modified":1450703430000},{"_id":"themes/light/source/css/_partial/comment.styl","hash":"6fa67d96903ac3b1674e6c8d4c801e16115808e0","modified":1450703430000},{"_id":"themes/light/source/css/_partial/footer.styl","hash":"821d50a9d45afec0274e1e64c2b37a71f3611c01","modified":1450703430000},{"_id":"themes/light/source/css/_partial/header.styl","hash":"ad67bf40a0cafb1e5514b058c853c517e602fe44","modified":1450703430000},{"_id":"themes/light/source/css/_partial/index.styl","hash":"eb0f1536a57cca57d9a280191fbb63185c266ee8","modified":1450703430000},{"_id":"themes/light/source/css/_partial/sidebar.styl","hash":"213a16c3206d045ca01b0abf8891e96421d61fb9","modified":1450703430000},{"_id":"themes/light/source/css/_partial/syntax.styl","hash":"bb6d6441aad58af730140bac83c880ac3050bd77","modified":1450703430000},{"_id":"themes/light/source/css/font/fontawesome-webfont.eot","hash":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1450703431000},{"_id":"themes/light/source/css/font/fontawesome-webfont.woff","hash":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1450703431000},{"_id":"themes/light/source/css/font/fontawesome-webfont.ttf","hash":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1450703431000},{"_id":"themes/light/source/css/font/fontawesome-webfont.svg","hash":"ff51bbb11dfe58345f41cead2c425d6e8be28176","modified":1450703431000},{"_id":"themes/light/.git/logs/refs/heads/master","hash":"40b45082115e59ecfcf9d18c8e3e2ad5cfb3c606","modified":1450703430000},{"_id":"themes/light/.git/refs/remotes/origin/HEAD","hash":"d9427cda09aba1cdde5c69c2b13c905bddb0bc51","modified":1450703430000},{"_id":"themes/light/.git/logs/refs/remotes/origin/HEAD","hash":"40b45082115e59ecfcf9d18c8e3e2ad5cfb3c606","modified":1450703430000},{"_id":"themes/light/.git/objects/pack/pack-1d269b2ca4df8c539fe0c39db96c64da0eb7dca1.pack","hash":"aa1b26ab50e4670b3923f9079cb9de760a79fdac","modified":1450703430000},{"_id":"public/baidusitemap.xml","hash":"6da0886a5e73e8faee61fc37117a8de4dc390482","modified":1505107299073},{"_id":"public/sitemap.xml","hash":"7bad835fc1dd961400028a3e90ae9e715cfcde7d","modified":1505107299369},{"_id":"public/index.html","hash":"07d2ac842fc5e18cfe4fc978f6847787593ade78","modified":1505107299397},{"_id":"public/about/index-chinese.html","hash":"8db5fb393855ca027173e07dbfd5b8416872aa4a","modified":1505107069534},{"_id":"public/about/index.html","hash":"425041c30ee6109d8d7a0db5f2c09e2f3aff780a","modified":1505107069535},{"_id":"public/new-page/index.html","hash":"438efd00ea7d76fee5a0e0846b81094df10d2fd3","modified":1505107069536},{"_id":"public/2016/01/10/Principle-of-real-time-video-transmission/index.html","hash":"4a77054a601508e4c060401104612cc747a5e5a5","modified":1505107069536},{"_id":"public/Blog/index.html","hash":"15ba4359d658c580e28c00e3bc12930d6df7fbfa","modified":1505107069536},{"_id":"public/2016/01/10/land-robot-already-open/index.html","hash":"4282cd998fb255dc30b2cedf6914cb862ba17e45","modified":1505107069536},{"_id":"public/2016/01/10/robot-control-principle/index.html","hash":"04d0ad111ee8ddfdeb7f97daa1806a3386b961b7","modified":1505107069536},{"_id":"public/2016/01/10/the-big-data-is-coming/index.html","hash":"9bc3e2210d09655dceb3eda0decd9191887b52f3","modified":1505107069536},{"_id":"public/2016/01/10/What-is-robot/index.html","hash":"27956797deb623448d456f2bcb19e87a8f1f70db","modified":1505107069536},{"_id":"public/categories/robot/index.html","hash":"e43d043826eb90ca4e151fd3577cdadf71a1d995","modified":1505107069536},{"_id":"public/2016/01/10/this-is-a-first-blog/index.html","hash":"2e76278103f5b3600132f4fe00a1ab2f4a3a2791","modified":1505107069536},{"_id":"public/categories/algorithm/index.html","hash":"12e7251bbec42ab6e741cc494a525b85cc741f63","modified":1505107069536},{"_id":"public/2017/08/29/DDNS/index.html","hash":"d7c3e7e8e05aac631f052bbace1d85a8ae69bb53","modified":1505107069536},{"_id":"public/categories/robot/Java/index.html","hash":"b522b7aa683a272ce9cdd54057c21c7fc16415af","modified":1505107069536},{"_id":"public/archives/index.html","hash":"d026d6d17766c0cf79f357b48e57f4397354c829","modified":1505107069536},{"_id":"public/categories/robot/Java/Android/index.html","hash":"99a9975aaa13bc0e231bd1b7fc5d8bd26721e61f","modified":1505107069537},{"_id":"public/archives/2016/index.html","hash":"9c9301a018364fd979b4882ac9cff8dffd6b47c1","modified":1505107069537},{"_id":"public/archives/2016/01/index.html","hash":"2820609f6649c7c869efff0afc0c0a6fdd3867b4","modified":1505107069537},{"_id":"public/archives/2017/index.html","hash":"366d580c01fc3952276ff65c95c95a171930ffe9","modified":1505107069537},{"_id":"public/archives/2017/08/index.html","hash":"9eef8ceb9d87420bfebe0ad5aa4be92a437fca7a","modified":1505107069537},{"_id":"public/tags/hello/index.html","hash":"c7d1f6a7af6984add4808ce7d6c4851296c2399d","modified":1505107069537},{"_id":"public/tags/robot/index.html","hash":"7c4f18022e5f4054e44fca2cc29d09554be22cbf","modified":1505107069537},{"_id":"public/tags/goal/index.html","hash":"70b2f826e3facf0221c5450d4512749645b961d9","modified":1505107069537},{"_id":"public/tags/helloworld/index.html","hash":"259b23fb4167b09b842d2af77b80839eb8f0112f","modified":1505107069537},{"_id":"public/tags/directions/index.html","hash":"2eed64d6e217bf42d4e8d714a4ab8cce21d32f93","modified":1505107069537},{"_id":"public/archives/2017/09/index.html","hash":"029065dd15e3edbc665980c54a1cd73301efb1b8","modified":1505107069546},{"_id":"public/tags/donation/index.html","hash":"7a2b7487547a7f68fb2a71a98c670eb6e17455b0","modified":1505107069546},{"_id":"public/tags/bigdata/index.html","hash":"26e03a63e2d05a58058a92bda0ce6f9464a2f06b","modified":1505107069546},{"_id":"public/tags/SAE/index.html","hash":"c6aa05494099a572bab6646fed530ad9e5466b14","modified":1505107069546},{"_id":"public/2017/09/06/SVO-SEMI-DIRECT-MONOCULAR-VISUAL-ODOMETRY-详解/index.html","hash":"5124719ce848d66153b816d97c4b73fb152bee29","modified":1505107299398},{"_id":"public/favicon.ico","hash":"a3affd9aa40692b5d9e617e37d407af82716d948","modified":1505107069553},{"_id":"public/CNAME","hash":"ae798f220c3c3a5a961c55be028adc35ba7d53e7","modified":1505107069553},{"_id":"public/robots.txt","hash":"05d15fa9cdd47d3f5ff815e474a8cc74e33234a9","modified":1505107069554},{"_id":"public/img/icon.png","hash":"d31a0b387102f1a8d0cfec93eed63aa99847ffc3","modified":1505107069554},{"_id":"public/fancybox/blank.gif","hash":"2daeaa8b5f19f0bc209d976c02bd6acb51b00b0a","modified":1505107069554},{"_id":"public/fancybox/fancybox_loading.gif","hash":"1a755fb2599f3a313cc6cfdb14df043f8c14a99c","modified":1505107069554},{"_id":"public/fancybox/fancybox_overlay.png","hash":"b3a4ee645ba494f52840ef8412015ba0f465dbe0","modified":1505107069554},{"_id":"public/fancybox/fancybox_loading@2x.gif","hash":"273b123496a42ba45c3416adb027cd99745058b0","modified":1505107069554},{"_id":"public/fancybox/fancybox_sprite.png","hash":"17df19f97628e77be09c352bf27425faea248251","modified":1505107069554},{"_id":"public/fancybox/fancybox_sprite@2x.png","hash":"30c58913f327e28f466a00f4c1ac8001b560aed8","modified":1505107069555},{"_id":"public/css/font/fontawesome-webfont.eot","hash":"d775f599ff3f23be082e6a9604b4898718923a37","modified":1505107069555},{"_id":"public/css/font/fontawesome-webfont.woff","hash":"0612cddf2f835cceffccc88fd194f97367d0b024","modified":1505107069555},{"_id":"public/css/font/fontawesome-webfont.ttf","hash":"a9468f6a1fe965fbcaf5a1bd6c11705e2fc5f84c","modified":1505107070344},{"_id":"public/css/font/fontawesome-webfont.svg","hash":"ff51bbb11dfe58345f41cead2c425d6e8be28176","modified":1505107070345},{"_id":"public/css/style.css","hash":"b7f0b73b0b8753a1da9814cc365f95121bcba98b","modified":1505107070355},{"_id":"public/fancybox/jquery.fancybox.css","hash":"5f163444617b6cf267342f06ac166a237bb62df9","modified":1505107070355},{"_id":"public/js/gallery.js","hash":"f8a4ba7fb8349cca374a3c69fff9b2bf21f742ed","modified":1505107070355},{"_id":"public/js/jquery.imagesloaded.min.js","hash":"4109837b1f6477bacc6b095a863b1b95b1b3693f","modified":1505107070355},{"_id":"public/fancybox/jquery.fancybox.pack.js","hash":"53360764b429c212f424399384417ccc233bb3be","modified":1505107070355}],"Category":[{"name":"robot","_id":"cj7fpvyp0000au2vxzwdan7rh"},{"name":"algorithm","_id":"cj7fpvyp9000fu2vx19q1hcpk"},{"name":"Java","parent":"cj7fpvyp0000au2vxzwdan7rh","_id":"cj7fpvypb000hu2vxbvvjo15n"},{"name":"Android","parent":"cj7fpvypb000hu2vxbvvjo15n","_id":"cj7fpvypj000mu2vxcvzooxx7"}],"Data":[],"Page":[{"_content":"<html>\n<head>\n<meta charset=\"UTF-8\" />\n<title>410</title>                                                                                                                                       \n</head>\n<body>\n<h1>404 Page Not Found</h1>\n<script type=\"text/javascript\" src=\"/imgr?src=http%3A%2F%2Fwww.qq.com%2F404%2Fsearch_children.js\" charset=\"utf-8\"></script>\n</body>\n</html>","source":"index.html","raw":"<html>\n<head>\n<meta charset=\"UTF-8\" />\n<title>410</title>                                                                                                                                       \n</head>\n<body>\n<h1>404 Page Not Found</h1>\n<script type=\"text/javascript\" src=\"/imgr?src=http%3A%2F%2Fwww.qq.com%2F404%2Fsearch_children.js\" charset=\"utf-8\"></script>\n</body>\n</html>","date":"2017-08-29T12:08:16.000Z","updated":"2016-01-11T02:52:44.000Z","path":"index.html","title":"","comments":1,"layout":"page","_id":"cj7fpvymm0000u2vxrmtb1cwq","content":"<html>\n<head>\n<meta charset=\"UTF-8\">\n<title>410</title>                                                                                                                                       \n</head>\n<body>\n<h1>404 Page Not Found</h1>\n<script type=\"text/javascript\" src=\"/imgr?src=http%3A%2F%2Fwww.qq.com%2F404%2Fsearch_children.js\" charset=\"utf-8\"></script>\n</body>\n</html>","site":{"data":{}},"excerpt":"","more":"<html>\n<head>\n<meta charset=\"UTF-8\">\n<title>410</title>                                                                                                                                       \n</head>\n<body>\n<h1>404 Page Not Found</h1>\n<script type=\"text/javascript\" src=\"/imgr?src=http%3A%2F%2Fwww.qq.com%2F404%2Fsearch_children.js\" charset=\"utf-8\"></script>\n</body>\n</html>"},{"title":"Blog","date":"2017-08-26T02:50:58.000Z","_content":"","source":"Blog/index.md","raw":"---\ntitle: Blog\ndate: 2017-08-26 10:50:58\n---\n","updated":"2017-08-26T02:50:58.000Z","path":"Blog/index.html","comments":1,"layout":"page","_id":"cj7fpvyo40002u2vx4dnpm4y4","content":"","site":{"data":{}},"excerpt":"","more":""},{"_content":"##团队介绍\n\n\nHiSource(嗨少)团队由一批年轻开源爱好者组成，成员囊括了信号处理、通信工程、计算机等相关专业领域的博士及硕士，团队拥有Linux驱动开发、硬件设计、Android/Web开发等一整套的技术产业链。团队以技术开源为先导，以自由共享为根基，以和谐开放为文化。基于智能硬件，团队研发并开源了四驱/履带式智能机器人、无线路由器、智能机械臂等一系列技术产品。不要问我们是谁，我们是茫茫人海中一批默默无闻的开源爱好者，我们的队伍，就是你的团队！加入我们，享受开源世界的乐趣！","source":"about/index-chinese.md","raw":"##团队介绍\n\n\nHiSource(嗨少)团队由一批年轻开源爱好者组成，成员囊括了信号处理、通信工程、计算机等相关专业领域的博士及硕士，团队拥有Linux驱动开发、硬件设计、Android/Web开发等一整套的技术产业链。团队以技术开源为先导，以自由共享为根基，以和谐开放为文化。基于智能硬件，团队研发并开源了四驱/履带式智能机器人、无线路由器、智能机械臂等一系列技术产品。不要问我们是谁，我们是茫茫人海中一批默默无闻的开源爱好者，我们的队伍，就是你的团队！加入我们，享受开源世界的乐趣！","date":"2017-08-29T12:08:16.000Z","updated":"2016-04-11T10:40:55.000Z","path":"about/index-chinese.html","title":"","comments":1,"layout":"page","_id":"cj7fpvyob0004u2vxyo0vx7t9","content":"<p>##团队介绍</p>\n<p>HiSource(嗨少)团队由一批年轻开源爱好者组成，成员囊括了信号处理、通信工程、计算机等相关专业领域的博士及硕士，团队拥有Linux驱动开发、硬件设计、Android/Web开发等一整套的技术产业链。团队以技术开源为先导，以自由共享为根基，以和谐开放为文化。基于智能硬件，团队研发并开源了四驱/履带式智能机器人、无线路由器、智能机械臂等一系列技术产品。不要问我们是谁，我们是茫茫人海中一批默默无闻的开源爱好者，我们的队伍，就是你的团队！加入我们，享受开源世界的乐趣！</p>\n","site":{"data":{}},"excerpt":"","more":"<p>##团队介绍</p>\n<p>HiSource(嗨少)团队由一批年轻开源爱好者组成，成员囊括了信号处理、通信工程、计算机等相关专业领域的博士及硕士，团队拥有Linux驱动开发、硬件设计、Android/Web开发等一整套的技术产业链。团队以技术开源为先导，以自由共享为根基，以和谐开放为文化。基于智能硬件，团队研发并开源了四驱/履带式智能机器人、无线路由器、智能机械臂等一系列技术产品。不要问我们是谁，我们是茫茫人海中一批默默无闻的开源爱好者，我们的队伍，就是你的团队！加入我们，享受开源世界的乐趣！</p>\n"},{"_content":"#THE TEAM\n\nOur team are constituted by four persons. We are all graduate student or above, young and full of energy. We are all very ordinary, but hava a great dream, through our efforts, so that learning Robot to become more easy!!!! Let's go to know our team.\n\nSunTong\n---\n\nGraduated from HoHai University of communication and information engineering, graduate degree. Our hardware designer, embedded software engineer,responsible for the hardware design of the robot, and the preparation of the bottom of the robot driver, good at C language programming, also is the captain of our team. \n\nChuZefan\n---\n\nGraduated from HoHai University of communication and information engineering, graduate degree. ","source":"about/index.md","raw":"#THE TEAM\n\nOur team are constituted by four persons. We are all graduate student or above, young and full of energy. We are all very ordinary, but hava a great dream, through our efforts, so that learning Robot to become more easy!!!! Let's go to know our team.\n\nSunTong\n---\n\nGraduated from HoHai University of communication and information engineering, graduate degree. Our hardware designer, embedded software engineer,responsible for the hardware design of the robot, and the preparation of the bottom of the robot driver, good at C language programming, also is the captain of our team. \n\nChuZefan\n---\n\nGraduated from HoHai University of communication and information engineering, graduate degree. ","date":"2017-08-29T12:08:16.000Z","updated":"2016-01-14T13:07:39.000Z","path":"about/index.html","title":"","comments":1,"layout":"page","_id":"cj7fpvyoh0006u2vxxjbwvsu0","content":"<p>#THE TEAM</p>\n<p>Our team are constituted by four persons. We are all graduate student or above, young and full of energy. We are all very ordinary, but hava a great dream, through our efforts, so that learning Robot to become more easy!!!! Let’s go to know our team.</p>\n<h2 id=\"SunTong\"><a href=\"#SunTong\" class=\"headerlink\" title=\"SunTong\"></a>SunTong</h2><p>Graduated from HoHai University of communication and information engineering, graduate degree. Our hardware designer, embedded software engineer,responsible for the hardware design of the robot, and the preparation of the bottom of the robot driver, good at C language programming, also is the captain of our team. </p>\n<h2 id=\"ChuZefan\"><a href=\"#ChuZefan\" class=\"headerlink\" title=\"ChuZefan\"></a>ChuZefan</h2><p>Graduated from HoHai University of communication and information engineering, graduate degree. </p>\n","site":{"data":{}},"excerpt":"","more":"<p>#THE TEAM</p>\n<p>Our team are constituted by four persons. We are all graduate student or above, young and full of energy. We are all very ordinary, but hava a great dream, through our efforts, so that learning Robot to become more easy!!!! Let’s go to know our team.</p>\n<h2 id=\"SunTong\"><a href=\"#SunTong\" class=\"headerlink\" title=\"SunTong\"></a>SunTong</h2><p>Graduated from HoHai University of communication and information engineering, graduate degree. Our hardware designer, embedded software engineer,responsible for the hardware design of the robot, and the preparation of the bottom of the robot driver, good at C language programming, also is the captain of our team. </p>\n<h2 id=\"ChuZefan\"><a href=\"#ChuZefan\" class=\"headerlink\" title=\"ChuZefan\"></a>ChuZefan</h2><p>Graduated from HoHai University of communication and information engineering, graduate degree. </p>\n"},{"date":"2016-01-10T05:10:56.000Z","comment":false,"_content":"\nToday, our team open a new blog to post our product, the robots of x-space.\n\nyou will be comunicate with it just only a Android Phone or Browser.\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**\n","source":"new-page/index.md","raw":"#This a  new blog\ndate: 2016-01-10 13:10:56\ncomment: false\n---\n\nToday, our team open a new blog to post our product, the robots of x-space.\n\nyou will be comunicate with it just only a Android Phone or Browser.\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**\n","updated":"2016-01-11T09:20:04.000Z","path":"new-page/index.html","title":"","comments":1,"layout":"page","_id":"cj7fpvyoy0008u2vxitywr37o","content":"<p>Today, our team open a new blog to post our product, the robots of x-space.</p>\n<p>you will be comunicate with it just only a Android Phone or Browser.</p>\n<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n","site":{"data":{}},"excerpt":"","more":"<p>Today, our team open a new blog to post our product, the robots of x-space.</p>\n<p>you will be comunicate with it just only a Android Phone or Browser.</p>\n<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n"}],"Post":[{"title":"SVO ( SEMI-DIRECT MONOCULAR VISUAL ODOMETRY)详解","date":"2017-09-06T13:18:27.000Z","_content":"SVO 简介\n---\n**SVO 全称 Semi-direct monocular Visual Odometry（半直接视觉里程计），是苏黎世大学机器人感知组的克里斯蒂安.弗斯特（[Christian Forster][Chris]，主页：Christian Forster）等人，于2014年ICRA会议上发表的工作，随后在github开源：[uzh-rpg/rpg_svo][github]。2016年扩展了多相机和IMU之后，写成期刊论文，称为SVO 2.0，预定将在IEEE Trans. on Robotics上发表（视频见[2]）。SVO 2.0目前未开源（个人认为以后也不会开)**<!--more-->\n\nSVO主要工作由弗斯特完成，此外他也在乔治亚理工的gtsam组呆过一段时间，参与了gtsam中IMU部分，文章发表在RSS 2015上，亦是VIO当中的著名工作[3]。此文章后续亦有期刊版本，预计也在TRO上发表。不过会议论文中公式推导有误，而且弗斯特本人似乎只参与了实现部分，没怎么管公式推导……当然这些都是八卦，不谈了。与SVO相关工作是同一组的REMODE[4]，实现了在SVO定位基础上的单目稠密建图（需要GPU），由SVO的二作马蒂亚（Matia Pizzoli）完成。\n\nSVO，虽然按照作者的理解，称为“半直接法”，然而按照我个人的理解，称为“稀疏直接法”可能更好一些。众所周知，VO主要分为特征点法和直接法。而SVO的实现中，混合使用了特征点与直接法：它跟踪了一些关键点（角点，没有描述子，由FAST实现），然后像直接法那样，根据这些关键点周围的信息，估计相机运动以及它们的位置。这与特征点法（以ORB-SLAM为代表）那样，需要对每张图像提取特征点和描述子的实现，是有明显不同的。所以，作者称之为“半直接法”。\n\n不过，由于SVO跟踪的“关键点”，亦可以理解成“梯度明显的像素”。从这个角度来看，它和LSD-SLAM[5]更加相近。只是LSD-SLAM跟踪了所有梯度明显的像素，形成半稠密地图；而SVO只跟踪稀疏的关键点，所以不妨称之为“稀疏直接法”。这一点，和今年慕尼黑理工丹尼尔.克莱默（Daniel Cremers）组的雅各布.恩格尔（Jakob Engel）提出的DSO[6]是很相似的。（PS：ORB,LSD,SVO几个作者似乎都去了同一个公司啊……那我还搞啥……）\n\n在视觉里程计中，直接法最突出的优点是非常快（ORB作者劳尔曾认为特征点比较快，我觉得是不对的）。而在直接法中（包括稀疏的，半稠密的以及稠密的），使用稀疏的直接法，既不必费力去计算描述子，也不必处理像稠密和半稠密那么多的信息，能够达到极快的速度。因此，SVO即使在低端计算平台上也能达到实时性，而在PC平台上则可以达到100多帧每秒的速度。在作者后续工作SVO 2.0中，速度更达到了惊人的400帧每秒。这使得SVO非常适用于计算平台受限的场合，例如无人机、手持AR/VR设备的定位。无人机也是弗斯特等人最初开发SVO的目标应用平台。\n\nSVO另一特点是实现了一种特有的深度滤波器（Depth Filter）。这里一种基于均匀——高斯混合分布的深度滤波器，由弗吉亚兹于2011年提取并推导[7]。由于原理较为复杂，之后再详细解释。SVO将这种滤波器用于关键点的深度估计，并使用了逆深度作为参数化形式，使之能够更好地计算特征点位置。这里SVO在建图线程中的主要任务。\n\n开源版的SVO代码清晰易读，十分适合读者作为第一个SLAM实例进行分析。初学者可以从SVO或ORB开始读起，弗斯特写代码一直比较清楚。\n\n**整个过程分为两个大模块：追踪与建图（与PTAM类似）。**\n- 上半部分为追踪部分。主要任务是估计当前帧的位姿。又分为两步：\n    + 先把当前帧和上一个追踪的帧进行比对，获取粗略的位姿。\n    + 然后根据粗略的位姿，将它与地图进行比对，得到精确的位姿并优化见到的地图点。随后判断此帧是否为关键帧。如果为关键帧就提取新的特征点，把这些点作为地图的种子点，放入优化线程。否则，不为关键帧的时候，就用此帧的信息更新地图中种子点的深度估计值。\n- 下半部分为建图部分。主要任务是估计特征点的深度。因为单目SLAM中，刚提的特征点是没有深度的，所以必须用新来的帧的信息，去更新这些特征点的深度分布，也就是所谓的“深度滤波器”。当某个点的深度收敛时，用它生成新的地图点，放进地图中，再被追踪部分使用。\n\n整个SVO架构要比ORB简单一些（ORB有三个线程，且要处理关键帧的共视关系和回环检测），所以效率也要高一些。下面详细谈这两个模块的做法。\n\n\n\n追踪（Tracking）部分\n-------------------\n追踪部分理解难点主要有两个：\n\n-如何计算帧与帧之间位姿变换？\n-如何计算帧与地图之间的位姿变换？\n\n下面分别来说这两点。\n\n1. Frame-to-Frame的位姿变换\n追踪的第一步是将当前帧与上一个追踪成功的帧进行对比，粗略估计当前帧的位姿。该问题的基本形式为：已知上一帧对地图点的观测（包括2D投影位置和深度），以及当前帧的图像，如何计算当前帧的位姿？用数学语言说，已经k-1帧的位姿T_{k-1}，并且知道它的观测量u_i, i=1, \\ldots, N时，求解T_{k-1,k}。\n\n在SVO里，该问题被称为 Model-based Image Alignment （带有相机模型的图像配准），实际上就是我们平时说的稀疏直接法。直接法的原理在我的博客中给出过比较细的推导：[直接法 – 半闲居士 – 博客园][blog] ，此外我也讲过一次讲座：[直接法的原理与实现_高翔_bilibili_演讲•公开课_科技_bilibili_哔哩哔哩弹幕视频网][bilibili] 。本质上，它通过最小化前一帧和当前帧之间的光度误差来求得当前帧的（粗略）位姿：\n\n要理解它，你需要非线性优化的基本知识。同时，为了求目标函数相对于位姿的导数，你需要学习一些李代数的知识。这在我的博客和讲座中均有比较详细的探讨：\n + [视觉slam第4章_演讲•公开课_科技_bilibili_哔哩哔哩弹幕视频网][bilibili2]\n + [非线性优化与g2o录像：http://pan.baidu.com/s/1c2qPdle][baidupan]\n\n由于不想在知乎打公式，所以请读者去看视频和讲座以了解其中原理（因为都已经讲过一遍了）。实现当中，SVO自己实现了高斯——牛顿法的迭代下降，并且比较取巧地使用了一种反向的求导方式：即雅可比在k-1帧图像上进行估计，而不在k帧上估计。这样做法的好处是在迭代过程中只需计算一次雅可比，其余部分只需更新残差即可（即G-N等式右侧的-J^Te）。这能够节省一定程度的计算量。另一个好处是，我们能够保证k-1帧的像素具有梯度，但没法保证在k帧上的投影也具有梯度，所以这样做至少能保证像素点的梯度是明显的。\n\n实现当中另一个需要注意的地方是金字塔的处理。这一步估计是从金字塔的顶层开始，把上一层的结果作为下一层估计的初始值，最后迭代到底层的。顶层的分辨率最小，所以这是一个由粗到精的过程（Coarse-to-Fine），使得在运动较大时也能有较好的结果。\n\n值得一提的是，完全可以使用优化库，例如g2o或ceres来实现所有的步骤。在优化库中，可以选用更多的优化方式，而且L-M或Dogleg的结果也会比G-N更有保证。我自己就用两个库各实现过一遍。\n\n2. Frame-to-Map\n在1求解之后，我们得到了当前帧位姿的粗略估计。因为它是基于上一帧的结果来计算的，所以如果把它当作真实位姿估计的话，将有较大的累积误差。因此，需要进一步和地图之间进行特征点比对，来对当前帧位姿进一步优化。主要步骤如下：\n\n+ 遍历地图中的所有点，计算在当前帧的投影位置。由于当前帧有粗略的位姿估计，这个投影位置应该与真实位置有少量误差（2~3个像素）。\n+ 对每个成功投影的地图点，比较这些点的初始观测图像与当前帧的图像。通过计算光度的误差，求取更精准的投影位置。这步类似于光流，在SVO中称为Refinement。\n+ 根据更精确的投影位置，进行位姿与地图点的优化。这一步类似于Bundle Adjustment，但SVO实现中，是把Pose和Point两个问题拆开优化的，速度更快。\n+ 判断是否生成关键帧，处理关键帧的生成。\n\n这里理解的难点是，地图点初次被观测到的图像与当前帧的图像进行比对时，不能直接对两个图像块求差，而需要计算一个仿射变换（Affine Warp）。这是因为初次观测和当前帧的位移较远，并且可能存在旋转，所以不能单纯地假设图像块不变。仿射变换的计算方式在PTAM论文的5.3节有介绍，似乎是一种比较标准的处理方式。（其实SVO的追踪部分和PTAM整个儿都挺像。）\n\n实现当中可能还需要注意一些细节。例如有些地方使用了网格，以保证观测点的均匀分布。还有Affine Warp当中需要注意特征点所在的金字塔层数，等等。\n\n此后的Bundle Adjustment部分和传统的区别不大，除了把pose和point分开计算之外。关键帧判断方面，SVO是比较薄弱的（考虑的东西太少），和ORB相比差了不少。\n\n\n\nMapping部分\n------\nMapping部分主要是计算特征点的深度。如前所言，单目VO中，刚刚从图像中提取的热乎的关键点是没有深度的，需要等相机位移之后再通过三角化，再估计这些点的深度。这些尚未具备有效深度信息的点，不妨称之为种子点（或候选点）。然而，三角化的成功与否（以及精度），取决于相机之间的平移量和视线的夹角，所以我们通常要维护种子点的深度分布，而不是单纯的一个深度值\n\n牵涉到概率分布的，往往都是理论一大堆屁话，实际可以操作的只有高斯分布一种——高斯只要在计算机里存均值和协方差即可。在逆深度[8]流行起来之后，用逆深度的高斯分布成了SLAM中的标配。然而SVO却使用了一种高斯——均匀混合分布的逆深度（由四个参数描述），推导并实现了它的更新方式，称为Depth Filter。它的工作逻辑大概是这样的：\n\n+ 如果进来一个关键帧，就提取关键帧上的新特征点，作为种子点放进一个种子队列中。\n+ 如果进来一个普通帧，就用普通帧的信息，更新所有种子点的概率分布。如果某个种子点的深度分布已经收敛，就把它放到地图中，供追踪线程使用。\n\n当然实现当中还有一些细节，比如删掉时间久远的种子点、删掉很少被看到的种子点等等。\n\n要理解Depth Filter，请搞清楚这两件事：\n    1. 基于高斯——均匀的滤波器，在理论上的推导是怎么样的？\n    2. Depth Filter又是如何利用普通帧的信息去更新种子点的？\n\n第1个问题，请参照论文[4],[7],以及[7]的补充材料，以及补充材料的补充材料。相信研究SVO的人应该都推导过，并不很难，静下心来推一遍即可，我当时也就一块小白板就推完了。在SLAM群的群文件里有一个depth filter.pdf，也给出了推导过程：\n或者请看[SVO原理解析 – 路游侠 – 博客园][SVO] 。我觉得应该用不着把公式在知乎上再敲一遍……\n\n第2个问题，你需要搞明白极线搜索这件事。由于种子点的深度不确定，它在别的帧里看起来就在一条直线（极线）上：\n于是你从这条极线的一个端点走到另一个端点，把每处的图像块都和参考的去比较，就可以（可能）找到正确的匹配了。哦别忘了要Affine Warp一下……找到之后，让depth filter更新其深度分布即可。当然如果位移太小或视线平行性太高，让深度变得更加不确定也是有可能的。在理想情况下，你可以期待一个地图点经过不断观测之后收敛的过程。\n\n评述\n---------------\n以上就是SVO的基本工作原理了。那么，这样一套系统实际工作起来效果如何呢？相比于其他几个开源方案有何优劣呢？\n\n首先要澄清一点的是：开源版本的SVO，是一个比较挫的版本。相比于LSD或ORB，我还很少看到有人能一次性把SVO跑通的。但是从论文上看，开源版本并不能代表SVO的真实水平。所以应该是心机弗斯特开源了一个只有部分代码的，不怎么好用的版本，仅供学习研究使用。相比之下，DSO，LSD，ORB至少能够在自己数据集上顺利运行，而ORB、LSD还能在大部分自定义的相机上运行，这点开源版本的SVO是做不到的。\n\n那么，抛开开源实现，从理论和框架上来说，SVO有何优劣呢？\n\n优点：\n\n+ 着实非常快，不愧为稀疏直接法；\n+ 关键点分布比较均匀；\n\n缺点：（不是我嫌弃它，确实有一堆可以吐槽的地方）\n1. 首先这货是VO，不是SLAM，没有闭环。这意味着丢失后没法重定位——丢了基本就挂了。\n2. 追踪部分：SVO首先将当前帧与上一个追踪的帧比较，以求得粗略的位姿估计。这里存在一个问题：这必须要求上一个帧是足够准确的！那么问题就来了：怎么知道上一个帧是准的呢？开源SVO里甚少考虑出错的情况。如果上一个帧由于遮挡、模糊等原因丢失，那么当前帧也就会得到一个错误的结果，导致和地图比对不上。又由于这货是没法重定位的，所以就。。。挂了呗。。。\n3. 还是追踪部分。既然是直接法，SVO就有直接法的所有缺点。后面那位同学来背一遍直接法缺点？\n\n    + 怕模糊（需要全局曝光相机）\n    + 怕大运动（图像非凸性）\n    + 怕光照变化（灰度不变假设）\n\n4. 地图部分：\n\n    + Depth Filter收敛较慢，结果比较严重地依赖于准确的位姿估计。如果统计收敛的种子点的比例，会发现并不高，很多计算浪费在不收敛的点上。\n    + 相比于纯高斯的逆深度，SVO的Depth Filter主要特点是能够通过Beta分布中的两个参数a,b来判断一个种子点是否为outlier。然而在特征点法中我们也能够通过描述来判断outlier，所以并不具有明显优势。\n\n\n小结\n---------------\n1. SVO是基于稀疏直接法的视觉里程计，速度非常快。\n2. 代码清晰易读，研究SVO会有不少启发。\n3. 但是开源实现存在诸多缺点，不实用。论文中效果应该不是这个开源代码能够实现的。\n\n\n参考文献\n[1] Foster et al., SVO: Fast semi-direct monocular visual odometry, ICRA 2014.\n[2] SVO 2.0\n[3] Foster et al., IMU preintegration on manifold for efficient visual-inertial maximum-a-posteriori estimation, RSS 2015.\n[4] Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide, REMODE: Probabilistic, monocular dense reconstruction in real time, ICRA 2014.\n[5] Engel, Jakob and Schops, Thomas and Cremers, Daniel, LSD-SLAM: Large-scale direct monocular SLAM, ECCV, 2014.\n[6] Engel, Jakob and Koltun, Vladlen and Cremers, Daniel, Direct sparse odometry, 2016.\n[7] George Vogiatzis and Carlos Hernández, Video-based, real-time multi-view stereo, Image and Vision Computing, 2011.\n[8] Civera, Javier and Davison, Andrew J and Montiel, JM Martinez, Inverse depth parametrization for monocular SLAM, IEEE transactions on robotics, 2008.\n\n附：SVO相关中文博客、材料\n[冯兵的博客|内外兼修 <一步步完善视觉里程计>是对SVO比较完整的介绍。][fb]\n[白巧克力亦唯心的博客 – 程序园][white] 贺一家的博客，有几篇关于SVO的介绍。\n[路游侠 – 博客园] [lu] 这个是谁。。。知道的回复我一下。。。\n[ORB-LSD-SVO比较-刘浩敏_bilibili_演讲•公开课_科技_bilibili_哔哩哔哩弹幕视频网][bilibili3] 浩敏师兄讲的一次ppt\n \n\n作者：半闲居士\n链接：https://www.zhihu.com/question/39904950/answer/138644975\n来源：知乎\n著作权归作者所有，转载请联系作者获得授权。\n\n\n[Chris]:http://www.cforster.ch/\n[github]:https://github.com/uzh-rpg/rpg_svo\n[blog]:http://www.cnblogs.com/gaoxiang12/p/5689927.html\n[bilibili]:http://www.bilibili.com/video/av6299156/\n[bilibili2]:http://www.bilibili.com/video/av7705856/\n[baidupan]:https://pan.baidu.com/s/1c2qPdle\n[SVO]:http://www.cnblogs.com/luyb/p/5773691.html\n[fb]:http://fengbing.net/\n[white]:http://www.voidcn.com/blog/wxzxur\n[lu]:http://www.cnblogs.com/luyb/\n[bilibili3]:http://www.bilibili.com/video/av5934066/\n\n\n\n\n\n\n","source":"_posts/SVO-SEMI-DIRECT-MONOCULAR-VISUAL-ODOMETRY-详解.md","raw":"---\ntitle: SVO ( SEMI-DIRECT MONOCULAR VISUAL ODOMETRY)详解\ndate: 2017-09-06 21:18:27\ntags:\n---\nSVO 简介\n---\n**SVO 全称 Semi-direct monocular Visual Odometry（半直接视觉里程计），是苏黎世大学机器人感知组的克里斯蒂安.弗斯特（[Christian Forster][Chris]，主页：Christian Forster）等人，于2014年ICRA会议上发表的工作，随后在github开源：[uzh-rpg/rpg_svo][github]。2016年扩展了多相机和IMU之后，写成期刊论文，称为SVO 2.0，预定将在IEEE Trans. on Robotics上发表（视频见[2]）。SVO 2.0目前未开源（个人认为以后也不会开)**<!--more-->\n\nSVO主要工作由弗斯特完成，此外他也在乔治亚理工的gtsam组呆过一段时间，参与了gtsam中IMU部分，文章发表在RSS 2015上，亦是VIO当中的著名工作[3]。此文章后续亦有期刊版本，预计也在TRO上发表。不过会议论文中公式推导有误，而且弗斯特本人似乎只参与了实现部分，没怎么管公式推导……当然这些都是八卦，不谈了。与SVO相关工作是同一组的REMODE[4]，实现了在SVO定位基础上的单目稠密建图（需要GPU），由SVO的二作马蒂亚（Matia Pizzoli）完成。\n\nSVO，虽然按照作者的理解，称为“半直接法”，然而按照我个人的理解，称为“稀疏直接法”可能更好一些。众所周知，VO主要分为特征点法和直接法。而SVO的实现中，混合使用了特征点与直接法：它跟踪了一些关键点（角点，没有描述子，由FAST实现），然后像直接法那样，根据这些关键点周围的信息，估计相机运动以及它们的位置。这与特征点法（以ORB-SLAM为代表）那样，需要对每张图像提取特征点和描述子的实现，是有明显不同的。所以，作者称之为“半直接法”。\n\n不过，由于SVO跟踪的“关键点”，亦可以理解成“梯度明显的像素”。从这个角度来看，它和LSD-SLAM[5]更加相近。只是LSD-SLAM跟踪了所有梯度明显的像素，形成半稠密地图；而SVO只跟踪稀疏的关键点，所以不妨称之为“稀疏直接法”。这一点，和今年慕尼黑理工丹尼尔.克莱默（Daniel Cremers）组的雅各布.恩格尔（Jakob Engel）提出的DSO[6]是很相似的。（PS：ORB,LSD,SVO几个作者似乎都去了同一个公司啊……那我还搞啥……）\n\n在视觉里程计中，直接法最突出的优点是非常快（ORB作者劳尔曾认为特征点比较快，我觉得是不对的）。而在直接法中（包括稀疏的，半稠密的以及稠密的），使用稀疏的直接法，既不必费力去计算描述子，也不必处理像稠密和半稠密那么多的信息，能够达到极快的速度。因此，SVO即使在低端计算平台上也能达到实时性，而在PC平台上则可以达到100多帧每秒的速度。在作者后续工作SVO 2.0中，速度更达到了惊人的400帧每秒。这使得SVO非常适用于计算平台受限的场合，例如无人机、手持AR/VR设备的定位。无人机也是弗斯特等人最初开发SVO的目标应用平台。\n\nSVO另一特点是实现了一种特有的深度滤波器（Depth Filter）。这里一种基于均匀——高斯混合分布的深度滤波器，由弗吉亚兹于2011年提取并推导[7]。由于原理较为复杂，之后再详细解释。SVO将这种滤波器用于关键点的深度估计，并使用了逆深度作为参数化形式，使之能够更好地计算特征点位置。这里SVO在建图线程中的主要任务。\n\n开源版的SVO代码清晰易读，十分适合读者作为第一个SLAM实例进行分析。初学者可以从SVO或ORB开始读起，弗斯特写代码一直比较清楚。\n\n**整个过程分为两个大模块：追踪与建图（与PTAM类似）。**\n- 上半部分为追踪部分。主要任务是估计当前帧的位姿。又分为两步：\n    + 先把当前帧和上一个追踪的帧进行比对，获取粗略的位姿。\n    + 然后根据粗略的位姿，将它与地图进行比对，得到精确的位姿并优化见到的地图点。随后判断此帧是否为关键帧。如果为关键帧就提取新的特征点，把这些点作为地图的种子点，放入优化线程。否则，不为关键帧的时候，就用此帧的信息更新地图中种子点的深度估计值。\n- 下半部分为建图部分。主要任务是估计特征点的深度。因为单目SLAM中，刚提的特征点是没有深度的，所以必须用新来的帧的信息，去更新这些特征点的深度分布，也就是所谓的“深度滤波器”。当某个点的深度收敛时，用它生成新的地图点，放进地图中，再被追踪部分使用。\n\n整个SVO架构要比ORB简单一些（ORB有三个线程，且要处理关键帧的共视关系和回环检测），所以效率也要高一些。下面详细谈这两个模块的做法。\n\n\n\n追踪（Tracking）部分\n-------------------\n追踪部分理解难点主要有两个：\n\n-如何计算帧与帧之间位姿变换？\n-如何计算帧与地图之间的位姿变换？\n\n下面分别来说这两点。\n\n1. Frame-to-Frame的位姿变换\n追踪的第一步是将当前帧与上一个追踪成功的帧进行对比，粗略估计当前帧的位姿。该问题的基本形式为：已知上一帧对地图点的观测（包括2D投影位置和深度），以及当前帧的图像，如何计算当前帧的位姿？用数学语言说，已经k-1帧的位姿T_{k-1}，并且知道它的观测量u_i, i=1, \\ldots, N时，求解T_{k-1,k}。\n\n在SVO里，该问题被称为 Model-based Image Alignment （带有相机模型的图像配准），实际上就是我们平时说的稀疏直接法。直接法的原理在我的博客中给出过比较细的推导：[直接法 – 半闲居士 – 博客园][blog] ，此外我也讲过一次讲座：[直接法的原理与实现_高翔_bilibili_演讲•公开课_科技_bilibili_哔哩哔哩弹幕视频网][bilibili] 。本质上，它通过最小化前一帧和当前帧之间的光度误差来求得当前帧的（粗略）位姿：\n\n要理解它，你需要非线性优化的基本知识。同时，为了求目标函数相对于位姿的导数，你需要学习一些李代数的知识。这在我的博客和讲座中均有比较详细的探讨：\n + [视觉slam第4章_演讲•公开课_科技_bilibili_哔哩哔哩弹幕视频网][bilibili2]\n + [非线性优化与g2o录像：http://pan.baidu.com/s/1c2qPdle][baidupan]\n\n由于不想在知乎打公式，所以请读者去看视频和讲座以了解其中原理（因为都已经讲过一遍了）。实现当中，SVO自己实现了高斯——牛顿法的迭代下降，并且比较取巧地使用了一种反向的求导方式：即雅可比在k-1帧图像上进行估计，而不在k帧上估计。这样做法的好处是在迭代过程中只需计算一次雅可比，其余部分只需更新残差即可（即G-N等式右侧的-J^Te）。这能够节省一定程度的计算量。另一个好处是，我们能够保证k-1帧的像素具有梯度，但没法保证在k帧上的投影也具有梯度，所以这样做至少能保证像素点的梯度是明显的。\n\n实现当中另一个需要注意的地方是金字塔的处理。这一步估计是从金字塔的顶层开始，把上一层的结果作为下一层估计的初始值，最后迭代到底层的。顶层的分辨率最小，所以这是一个由粗到精的过程（Coarse-to-Fine），使得在运动较大时也能有较好的结果。\n\n值得一提的是，完全可以使用优化库，例如g2o或ceres来实现所有的步骤。在优化库中，可以选用更多的优化方式，而且L-M或Dogleg的结果也会比G-N更有保证。我自己就用两个库各实现过一遍。\n\n2. Frame-to-Map\n在1求解之后，我们得到了当前帧位姿的粗略估计。因为它是基于上一帧的结果来计算的，所以如果把它当作真实位姿估计的话，将有较大的累积误差。因此，需要进一步和地图之间进行特征点比对，来对当前帧位姿进一步优化。主要步骤如下：\n\n+ 遍历地图中的所有点，计算在当前帧的投影位置。由于当前帧有粗略的位姿估计，这个投影位置应该与真实位置有少量误差（2~3个像素）。\n+ 对每个成功投影的地图点，比较这些点的初始观测图像与当前帧的图像。通过计算光度的误差，求取更精准的投影位置。这步类似于光流，在SVO中称为Refinement。\n+ 根据更精确的投影位置，进行位姿与地图点的优化。这一步类似于Bundle Adjustment，但SVO实现中，是把Pose和Point两个问题拆开优化的，速度更快。\n+ 判断是否生成关键帧，处理关键帧的生成。\n\n这里理解的难点是，地图点初次被观测到的图像与当前帧的图像进行比对时，不能直接对两个图像块求差，而需要计算一个仿射变换（Affine Warp）。这是因为初次观测和当前帧的位移较远，并且可能存在旋转，所以不能单纯地假设图像块不变。仿射变换的计算方式在PTAM论文的5.3节有介绍，似乎是一种比较标准的处理方式。（其实SVO的追踪部分和PTAM整个儿都挺像。）\n\n实现当中可能还需要注意一些细节。例如有些地方使用了网格，以保证观测点的均匀分布。还有Affine Warp当中需要注意特征点所在的金字塔层数，等等。\n\n此后的Bundle Adjustment部分和传统的区别不大，除了把pose和point分开计算之外。关键帧判断方面，SVO是比较薄弱的（考虑的东西太少），和ORB相比差了不少。\n\n\n\nMapping部分\n------\nMapping部分主要是计算特征点的深度。如前所言，单目VO中，刚刚从图像中提取的热乎的关键点是没有深度的，需要等相机位移之后再通过三角化，再估计这些点的深度。这些尚未具备有效深度信息的点，不妨称之为种子点（或候选点）。然而，三角化的成功与否（以及精度），取决于相机之间的平移量和视线的夹角，所以我们通常要维护种子点的深度分布，而不是单纯的一个深度值\n\n牵涉到概率分布的，往往都是理论一大堆屁话，实际可以操作的只有高斯分布一种——高斯只要在计算机里存均值和协方差即可。在逆深度[8]流行起来之后，用逆深度的高斯分布成了SLAM中的标配。然而SVO却使用了一种高斯——均匀混合分布的逆深度（由四个参数描述），推导并实现了它的更新方式，称为Depth Filter。它的工作逻辑大概是这样的：\n\n+ 如果进来一个关键帧，就提取关键帧上的新特征点，作为种子点放进一个种子队列中。\n+ 如果进来一个普通帧，就用普通帧的信息，更新所有种子点的概率分布。如果某个种子点的深度分布已经收敛，就把它放到地图中，供追踪线程使用。\n\n当然实现当中还有一些细节，比如删掉时间久远的种子点、删掉很少被看到的种子点等等。\n\n要理解Depth Filter，请搞清楚这两件事：\n    1. 基于高斯——均匀的滤波器，在理论上的推导是怎么样的？\n    2. Depth Filter又是如何利用普通帧的信息去更新种子点的？\n\n第1个问题，请参照论文[4],[7],以及[7]的补充材料，以及补充材料的补充材料。相信研究SVO的人应该都推导过，并不很难，静下心来推一遍即可，我当时也就一块小白板就推完了。在SLAM群的群文件里有一个depth filter.pdf，也给出了推导过程：\n或者请看[SVO原理解析 – 路游侠 – 博客园][SVO] 。我觉得应该用不着把公式在知乎上再敲一遍……\n\n第2个问题，你需要搞明白极线搜索这件事。由于种子点的深度不确定，它在别的帧里看起来就在一条直线（极线）上：\n于是你从这条极线的一个端点走到另一个端点，把每处的图像块都和参考的去比较，就可以（可能）找到正确的匹配了。哦别忘了要Affine Warp一下……找到之后，让depth filter更新其深度分布即可。当然如果位移太小或视线平行性太高，让深度变得更加不确定也是有可能的。在理想情况下，你可以期待一个地图点经过不断观测之后收敛的过程。\n\n评述\n---------------\n以上就是SVO的基本工作原理了。那么，这样一套系统实际工作起来效果如何呢？相比于其他几个开源方案有何优劣呢？\n\n首先要澄清一点的是：开源版本的SVO，是一个比较挫的版本。相比于LSD或ORB，我还很少看到有人能一次性把SVO跑通的。但是从论文上看，开源版本并不能代表SVO的真实水平。所以应该是心机弗斯特开源了一个只有部分代码的，不怎么好用的版本，仅供学习研究使用。相比之下，DSO，LSD，ORB至少能够在自己数据集上顺利运行，而ORB、LSD还能在大部分自定义的相机上运行，这点开源版本的SVO是做不到的。\n\n那么，抛开开源实现，从理论和框架上来说，SVO有何优劣呢？\n\n优点：\n\n+ 着实非常快，不愧为稀疏直接法；\n+ 关键点分布比较均匀；\n\n缺点：（不是我嫌弃它，确实有一堆可以吐槽的地方）\n1. 首先这货是VO，不是SLAM，没有闭环。这意味着丢失后没法重定位——丢了基本就挂了。\n2. 追踪部分：SVO首先将当前帧与上一个追踪的帧比较，以求得粗略的位姿估计。这里存在一个问题：这必须要求上一个帧是足够准确的！那么问题就来了：怎么知道上一个帧是准的呢？开源SVO里甚少考虑出错的情况。如果上一个帧由于遮挡、模糊等原因丢失，那么当前帧也就会得到一个错误的结果，导致和地图比对不上。又由于这货是没法重定位的，所以就。。。挂了呗。。。\n3. 还是追踪部分。既然是直接法，SVO就有直接法的所有缺点。后面那位同学来背一遍直接法缺点？\n\n    + 怕模糊（需要全局曝光相机）\n    + 怕大运动（图像非凸性）\n    + 怕光照变化（灰度不变假设）\n\n4. 地图部分：\n\n    + Depth Filter收敛较慢，结果比较严重地依赖于准确的位姿估计。如果统计收敛的种子点的比例，会发现并不高，很多计算浪费在不收敛的点上。\n    + 相比于纯高斯的逆深度，SVO的Depth Filter主要特点是能够通过Beta分布中的两个参数a,b来判断一个种子点是否为outlier。然而在特征点法中我们也能够通过描述来判断outlier，所以并不具有明显优势。\n\n\n小结\n---------------\n1. SVO是基于稀疏直接法的视觉里程计，速度非常快。\n2. 代码清晰易读，研究SVO会有不少启发。\n3. 但是开源实现存在诸多缺点，不实用。论文中效果应该不是这个开源代码能够实现的。\n\n\n参考文献\n[1] Foster et al., SVO: Fast semi-direct monocular visual odometry, ICRA 2014.\n[2] SVO 2.0\n[3] Foster et al., IMU preintegration on manifold for efficient visual-inertial maximum-a-posteriori estimation, RSS 2015.\n[4] Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide, REMODE: Probabilistic, monocular dense reconstruction in real time, ICRA 2014.\n[5] Engel, Jakob and Schops, Thomas and Cremers, Daniel, LSD-SLAM: Large-scale direct monocular SLAM, ECCV, 2014.\n[6] Engel, Jakob and Koltun, Vladlen and Cremers, Daniel, Direct sparse odometry, 2016.\n[7] George Vogiatzis and Carlos Hernández, Video-based, real-time multi-view stereo, Image and Vision Computing, 2011.\n[8] Civera, Javier and Davison, Andrew J and Montiel, JM Martinez, Inverse depth parametrization for monocular SLAM, IEEE transactions on robotics, 2008.\n\n附：SVO相关中文博客、材料\n[冯兵的博客|内外兼修 <一步步完善视觉里程计>是对SVO比较完整的介绍。][fb]\n[白巧克力亦唯心的博客 – 程序园][white] 贺一家的博客，有几篇关于SVO的介绍。\n[路游侠 – 博客园] [lu] 这个是谁。。。知道的回复我一下。。。\n[ORB-LSD-SVO比较-刘浩敏_bilibili_演讲•公开课_科技_bilibili_哔哩哔哩弹幕视频网][bilibili3] 浩敏师兄讲的一次ppt\n \n\n作者：半闲居士\n链接：https://www.zhihu.com/question/39904950/answer/138644975\n来源：知乎\n著作权归作者所有，转载请联系作者获得授权。\n\n\n[Chris]:http://www.cforster.ch/\n[github]:https://github.com/uzh-rpg/rpg_svo\n[blog]:http://www.cnblogs.com/gaoxiang12/p/5689927.html\n[bilibili]:http://www.bilibili.com/video/av6299156/\n[bilibili2]:http://www.bilibili.com/video/av7705856/\n[baidupan]:https://pan.baidu.com/s/1c2qPdle\n[SVO]:http://www.cnblogs.com/luyb/p/5773691.html\n[fb]:http://fengbing.net/\n[white]:http://www.voidcn.com/blog/wxzxur\n[lu]:http://www.cnblogs.com/luyb/\n[bilibili3]:http://www.bilibili.com/video/av5934066/\n\n\n\n\n\n\n","slug":"SVO-SEMI-DIRECT-MONOCULAR-VISUAL-ODOMETRY-详解","published":1,"updated":"2017-09-11T05:23:56.000Z","_id":"cj7fpvyny0001u2vxx4rw1rnz","comments":1,"layout":"post","photos":[],"link":"","content":"<h2 id=\"SVO-简介\"><a href=\"#SVO-简介\" class=\"headerlink\" title=\"SVO 简介\"></a>SVO 简介</h2><p><strong>SVO 全称 Semi-direct monocular Visual Odometry（半直接视觉里程计），是苏黎世大学机器人感知组的克里斯蒂安.弗斯特（<a href=\"http://www.cforster.ch/\" target=\"_blank\" rel=\"external\">Christian Forster</a>，主页：Christian Forster）等人，于2014年ICRA会议上发表的工作，随后在github开源：<a href=\"https://github.com/uzh-rpg/rpg_svo\" target=\"_blank\" rel=\"external\">uzh-rpg/rpg_svo</a>。2016年扩展了多相机和IMU之后，写成期刊论文，称为SVO 2.0，预定将在IEEE Trans. on Robotics上发表（视频见[2]）。SVO 2.0目前未开源（个人认为以后也不会开)</strong><a id=\"more\"></a></p>\n<p>SVO主要工作由弗斯特完成，此外他也在乔治亚理工的gtsam组呆过一段时间，参与了gtsam中IMU部分，文章发表在RSS 2015上，亦是VIO当中的著名工作[3]。此文章后续亦有期刊版本，预计也在TRO上发表。不过会议论文中公式推导有误，而且弗斯特本人似乎只参与了实现部分，没怎么管公式推导……当然这些都是八卦，不谈了。与SVO相关工作是同一组的REMODE[4]，实现了在SVO定位基础上的单目稠密建图（需要GPU），由SVO的二作马蒂亚（Matia Pizzoli）完成。</p>\n<p>SVO，虽然按照作者的理解，称为“半直接法”，然而按照我个人的理解，称为“稀疏直接法”可能更好一些。众所周知，VO主要分为特征点法和直接法。而SVO的实现中，混合使用了特征点与直接法：它跟踪了一些关键点（角点，没有描述子，由FAST实现），然后像直接法那样，根据这些关键点周围的信息，估计相机运动以及它们的位置。这与特征点法（以ORB-SLAM为代表）那样，需要对每张图像提取特征点和描述子的实现，是有明显不同的。所以，作者称之为“半直接法”。</p>\n<p>不过，由于SVO跟踪的“关键点”，亦可以理解成“梯度明显的像素”。从这个角度来看，它和LSD-SLAM[5]更加相近。只是LSD-SLAM跟踪了所有梯度明显的像素，形成半稠密地图；而SVO只跟踪稀疏的关键点，所以不妨称之为“稀疏直接法”。这一点，和今年慕尼黑理工丹尼尔.克莱默（Daniel Cremers）组的雅各布.恩格尔（Jakob Engel）提出的DSO[6]是很相似的。（PS：ORB,LSD,SVO几个作者似乎都去了同一个公司啊……那我还搞啥……）</p>\n<p>在视觉里程计中，直接法最突出的优点是非常快（ORB作者劳尔曾认为特征点比较快，我觉得是不对的）。而在直接法中（包括稀疏的，半稠密的以及稠密的），使用稀疏的直接法，既不必费力去计算描述子，也不必处理像稠密和半稠密那么多的信息，能够达到极快的速度。因此，SVO即使在低端计算平台上也能达到实时性，而在PC平台上则可以达到100多帧每秒的速度。在作者后续工作SVO 2.0中，速度更达到了惊人的400帧每秒。这使得SVO非常适用于计算平台受限的场合，例如无人机、手持AR/VR设备的定位。无人机也是弗斯特等人最初开发SVO的目标应用平台。</p>\n<p>SVO另一特点是实现了一种特有的深度滤波器（Depth Filter）。这里一种基于均匀——高斯混合分布的深度滤波器，由弗吉亚兹于2011年提取并推导[7]。由于原理较为复杂，之后再详细解释。SVO将这种滤波器用于关键点的深度估计，并使用了逆深度作为参数化形式，使之能够更好地计算特征点位置。这里SVO在建图线程中的主要任务。</p>\n<p>开源版的SVO代码清晰易读，十分适合读者作为第一个SLAM实例进行分析。初学者可以从SVO或ORB开始读起，弗斯特写代码一直比较清楚。</p>\n<p><strong>整个过程分为两个大模块：追踪与建图（与PTAM类似）。</strong></p>\n<ul>\n<li>上半部分为追踪部分。主要任务是估计当前帧的位姿。又分为两步：<ul>\n<li>先把当前帧和上一个追踪的帧进行比对，获取粗略的位姿。</li>\n<li>然后根据粗略的位姿，将它与地图进行比对，得到精确的位姿并优化见到的地图点。随后判断此帧是否为关键帧。如果为关键帧就提取新的特征点，把这些点作为地图的种子点，放入优化线程。否则，不为关键帧的时候，就用此帧的信息更新地图中种子点的深度估计值。</li>\n</ul>\n</li>\n<li>下半部分为建图部分。主要任务是估计特征点的深度。因为单目SLAM中，刚提的特征点是没有深度的，所以必须用新来的帧的信息，去更新这些特征点的深度分布，也就是所谓的“深度滤波器”。当某个点的深度收敛时，用它生成新的地图点，放进地图中，再被追踪部分使用。</li>\n</ul>\n<p>整个SVO架构要比ORB简单一些（ORB有三个线程，且要处理关键帧的共视关系和回环检测），所以效率也要高一些。下面详细谈这两个模块的做法。</p>\n<h2 id=\"追踪（Tracking）部分\"><a href=\"#追踪（Tracking）部分\" class=\"headerlink\" title=\"追踪（Tracking）部分\"></a>追踪（Tracking）部分</h2><p>追踪部分理解难点主要有两个：</p>\n<p>-如何计算帧与帧之间位姿变换？<br>-如何计算帧与地图之间的位姿变换？</p>\n<p>下面分别来说这两点。</p>\n<ol>\n<li>Frame-to-Frame的位姿变换<br>追踪的第一步是将当前帧与上一个追踪成功的帧进行对比，粗略估计当前帧的位姿。该问题的基本形式为：已知上一帧对地图点的观测（包括2D投影位置和深度），以及当前帧的图像，如何计算当前帧的位姿？用数学语言说，已经k-1帧的位姿T_{k-1}，并且知道它的观测量u<em>i, i=1, \\ldots, N时，求解T</em>{k-1,k}。</li>\n</ol>\n<p>在SVO里，该问题被称为 Model-based Image Alignment （带有相机模型的图像配准），实际上就是我们平时说的稀疏直接法。直接法的原理在我的博客中给出过比较细的推导：<a href=\"http://www.cnblogs.com/gaoxiang12/p/5689927.html\" target=\"_blank\" rel=\"external\">直接法 – 半闲居士 – 博客园</a> ，此外我也讲过一次讲座：<a href=\"http://www.bilibili.com/video/av6299156/\" target=\"_blank\" rel=\"external\">直接法的原理与实现_高翔<em>bilibili</em>演讲•公开课_科技<em>bilibili</em>哔哩哔哩弹幕视频网</a> 。本质上，它通过最小化前一帧和当前帧之间的光度误差来求得当前帧的（粗略）位姿：</p>\n<p>要理解它，你需要非线性优化的基本知识。同时，为了求目标函数相对于位姿的导数，你需要学习一些李代数的知识。这在我的博客和讲座中均有比较详细的探讨：</p>\n<ul>\n<li><a href=\"http://www.bilibili.com/video/av7705856/\" target=\"_blank\" rel=\"external\">视觉slam第4章<em>演讲•公开课</em>科技<em>bilibili</em>哔哩哔哩弹幕视频网</a></li>\n<li><a href=\"https://pan.baidu.com/s/1c2qPdle\" target=\"_blank\" rel=\"external\">非线性优化与g2o录像：http://pan.baidu.com/s/1c2qPdle</a></li>\n</ul>\n<p>由于不想在知乎打公式，所以请读者去看视频和讲座以了解其中原理（因为都已经讲过一遍了）。实现当中，SVO自己实现了高斯——牛顿法的迭代下降，并且比较取巧地使用了一种反向的求导方式：即雅可比在k-1帧图像上进行估计，而不在k帧上估计。这样做法的好处是在迭代过程中只需计算一次雅可比，其余部分只需更新残差即可（即G-N等式右侧的-J^Te）。这能够节省一定程度的计算量。另一个好处是，我们能够保证k-1帧的像素具有梯度，但没法保证在k帧上的投影也具有梯度，所以这样做至少能保证像素点的梯度是明显的。</p>\n<p>实现当中另一个需要注意的地方是金字塔的处理。这一步估计是从金字塔的顶层开始，把上一层的结果作为下一层估计的初始值，最后迭代到底层的。顶层的分辨率最小，所以这是一个由粗到精的过程（Coarse-to-Fine），使得在运动较大时也能有较好的结果。</p>\n<p>值得一提的是，完全可以使用优化库，例如g2o或ceres来实现所有的步骤。在优化库中，可以选用更多的优化方式，而且L-M或Dogleg的结果也会比G-N更有保证。我自己就用两个库各实现过一遍。</p>\n<ol>\n<li>Frame-to-Map<br>在1求解之后，我们得到了当前帧位姿的粗略估计。因为它是基于上一帧的结果来计算的，所以如果把它当作真实位姿估计的话，将有较大的累积误差。因此，需要进一步和地图之间进行特征点比对，来对当前帧位姿进一步优化。主要步骤如下：</li>\n</ol>\n<ul>\n<li>遍历地图中的所有点，计算在当前帧的投影位置。由于当前帧有粗略的位姿估计，这个投影位置应该与真实位置有少量误差（2~3个像素）。</li>\n<li>对每个成功投影的地图点，比较这些点的初始观测图像与当前帧的图像。通过计算光度的误差，求取更精准的投影位置。这步类似于光流，在SVO中称为Refinement。</li>\n<li>根据更精确的投影位置，进行位姿与地图点的优化。这一步类似于Bundle Adjustment，但SVO实现中，是把Pose和Point两个问题拆开优化的，速度更快。</li>\n<li>判断是否生成关键帧，处理关键帧的生成。</li>\n</ul>\n<p>这里理解的难点是，地图点初次被观测到的图像与当前帧的图像进行比对时，不能直接对两个图像块求差，而需要计算一个仿射变换（Affine Warp）。这是因为初次观测和当前帧的位移较远，并且可能存在旋转，所以不能单纯地假设图像块不变。仿射变换的计算方式在PTAM论文的5.3节有介绍，似乎是一种比较标准的处理方式。（其实SVO的追踪部分和PTAM整个儿都挺像。）</p>\n<p>实现当中可能还需要注意一些细节。例如有些地方使用了网格，以保证观测点的均匀分布。还有Affine Warp当中需要注意特征点所在的金字塔层数，等等。</p>\n<p>此后的Bundle Adjustment部分和传统的区别不大，除了把pose和point分开计算之外。关键帧判断方面，SVO是比较薄弱的（考虑的东西太少），和ORB相比差了不少。</p>\n<h2 id=\"Mapping部分\"><a href=\"#Mapping部分\" class=\"headerlink\" title=\"Mapping部分\"></a>Mapping部分</h2><p>Mapping部分主要是计算特征点的深度。如前所言，单目VO中，刚刚从图像中提取的热乎的关键点是没有深度的，需要等相机位移之后再通过三角化，再估计这些点的深度。这些尚未具备有效深度信息的点，不妨称之为种子点（或候选点）。然而，三角化的成功与否（以及精度），取决于相机之间的平移量和视线的夹角，所以我们通常要维护种子点的深度分布，而不是单纯的一个深度值</p>\n<p>牵涉到概率分布的，往往都是理论一大堆屁话，实际可以操作的只有高斯分布一种——高斯只要在计算机里存均值和协方差即可。在逆深度[8]流行起来之后，用逆深度的高斯分布成了SLAM中的标配。然而SVO却使用了一种高斯——均匀混合分布的逆深度（由四个参数描述），推导并实现了它的更新方式，称为Depth Filter。它的工作逻辑大概是这样的：</p>\n<ul>\n<li>如果进来一个关键帧，就提取关键帧上的新特征点，作为种子点放进一个种子队列中。</li>\n<li>如果进来一个普通帧，就用普通帧的信息，更新所有种子点的概率分布。如果某个种子点的深度分布已经收敛，就把它放到地图中，供追踪线程使用。</li>\n</ul>\n<p>当然实现当中还有一些细节，比如删掉时间久远的种子点、删掉很少被看到的种子点等等。</p>\n<p>要理解Depth Filter，请搞清楚这两件事：</p>\n<pre><code>1. 基于高斯——均匀的滤波器，在理论上的推导是怎么样的？\n2. Depth Filter又是如何利用普通帧的信息去更新种子点的？\n</code></pre><p>第1个问题，请参照论文[4],[7],以及[7]的补充材料，以及补充材料的补充材料。相信研究SVO的人应该都推导过，并不很难，静下心来推一遍即可，我当时也就一块小白板就推完了。在SLAM群的群文件里有一个depth filter.pdf，也给出了推导过程：<br>或者请看<a href=\"http://www.cnblogs.com/luyb/p/5773691.html\" target=\"_blank\" rel=\"external\">SVO原理解析 – 路游侠 – 博客园</a> 。我觉得应该用不着把公式在知乎上再敲一遍……</p>\n<p>第2个问题，你需要搞明白极线搜索这件事。由于种子点的深度不确定，它在别的帧里看起来就在一条直线（极线）上：<br>于是你从这条极线的一个端点走到另一个端点，把每处的图像块都和参考的去比较，就可以（可能）找到正确的匹配了。哦别忘了要Affine Warp一下……找到之后，让depth filter更新其深度分布即可。当然如果位移太小或视线平行性太高，让深度变得更加不确定也是有可能的。在理想情况下，你可以期待一个地图点经过不断观测之后收敛的过程。</p>\n<h2 id=\"评述\"><a href=\"#评述\" class=\"headerlink\" title=\"评述\"></a>评述</h2><p>以上就是SVO的基本工作原理了。那么，这样一套系统实际工作起来效果如何呢？相比于其他几个开源方案有何优劣呢？</p>\n<p>首先要澄清一点的是：开源版本的SVO，是一个比较挫的版本。相比于LSD或ORB，我还很少看到有人能一次性把SVO跑通的。但是从论文上看，开源版本并不能代表SVO的真实水平。所以应该是心机弗斯特开源了一个只有部分代码的，不怎么好用的版本，仅供学习研究使用。相比之下，DSO，LSD，ORB至少能够在自己数据集上顺利运行，而ORB、LSD还能在大部分自定义的相机上运行，这点开源版本的SVO是做不到的。</p>\n<p>那么，抛开开源实现，从理论和框架上来说，SVO有何优劣呢？</p>\n<p>优点：</p>\n<ul>\n<li>着实非常快，不愧为稀疏直接法；</li>\n<li>关键点分布比较均匀；</li>\n</ul>\n<p>缺点：（不是我嫌弃它，确实有一堆可以吐槽的地方）</p>\n<ol>\n<li>首先这货是VO，不是SLAM，没有闭环。这意味着丢失后没法重定位——丢了基本就挂了。</li>\n<li>追踪部分：SVO首先将当前帧与上一个追踪的帧比较，以求得粗略的位姿估计。这里存在一个问题：这必须要求上一个帧是足够准确的！那么问题就来了：怎么知道上一个帧是准的呢？开源SVO里甚少考虑出错的情况。如果上一个帧由于遮挡、模糊等原因丢失，那么当前帧也就会得到一个错误的结果，导致和地图比对不上。又由于这货是没法重定位的，所以就。。。挂了呗。。。</li>\n<li><p>还是追踪部分。既然是直接法，SVO就有直接法的所有缺点。后面那位同学来背一遍直接法缺点？</p>\n<ul>\n<li>怕模糊（需要全局曝光相机）</li>\n<li>怕大运动（图像非凸性）</li>\n<li>怕光照变化（灰度不变假设）</li>\n</ul>\n</li>\n<li><p>地图部分：</p>\n<ul>\n<li>Depth Filter收敛较慢，结果比较严重地依赖于准确的位姿估计。如果统计收敛的种子点的比例，会发现并不高，很多计算浪费在不收敛的点上。</li>\n<li>相比于纯高斯的逆深度，SVO的Depth Filter主要特点是能够通过Beta分布中的两个参数a,b来判断一个种子点是否为outlier。然而在特征点法中我们也能够通过描述来判断outlier，所以并不具有明显优势。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><ol>\n<li>SVO是基于稀疏直接法的视觉里程计，速度非常快。</li>\n<li>代码清晰易读，研究SVO会有不少启发。</li>\n<li>但是开源实现存在诸多缺点，不实用。论文中效果应该不是这个开源代码能够实现的。</li>\n</ol>\n<p>参考文献<br>[1] Foster et al., SVO: Fast semi-direct monocular visual odometry, ICRA 2014.<br>[2] SVO 2.0<br>[3] Foster et al., IMU preintegration on manifold for efficient visual-inertial maximum-a-posteriori estimation, RSS 2015.<br>[4] Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide, REMODE: Probabilistic, monocular dense reconstruction in real time, ICRA 2014.<br>[5] Engel, Jakob and Schops, Thomas and Cremers, Daniel, LSD-SLAM: Large-scale direct monocular SLAM, ECCV, 2014.<br>[6] Engel, Jakob and Koltun, Vladlen and Cremers, Daniel, Direct sparse odometry, 2016.<br>[7] George Vogiatzis and Carlos Hernández, Video-based, real-time multi-view stereo, Image and Vision Computing, 2011.<br>[8] Civera, Javier and Davison, Andrew J and Montiel, JM Martinez, Inverse depth parametrization for monocular SLAM, IEEE transactions on robotics, 2008.</p>\n<p>附：SVO相关中文博客、材料<br><a href=\"http://fengbing.net/\" target=\"_blank\" rel=\"external\">冯兵的博客|内外兼修 &lt;一步步完善视觉里程计&gt;是对SVO比较完整的介绍。</a><br><a href=\"http://www.voidcn.com/blog/wxzxur\" target=\"_blank\" rel=\"external\">白巧克力亦唯心的博客 – 程序园</a> 贺一家的博客，有几篇关于SVO的介绍。<br><a href=\"http://www.cnblogs.com/luyb/\" target=\"_blank\" rel=\"external\">路游侠 – 博客园</a> 这个是谁。。。知道的回复我一下。。。<br><a href=\"http://www.bilibili.com/video/av5934066/\" target=\"_blank\" rel=\"external\">ORB-LSD-SVO比较-刘浩敏<em>bilibili</em>演讲•公开课_科技<em>bilibili</em>哔哩哔哩弹幕视频网</a> 浩敏师兄讲的一次ppt</p>\n<p>作者：半闲居士<br>链接：<a href=\"https://www.zhihu.com/question/39904950/answer/138644975\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/39904950/answer/138644975</a><br>来源：知乎<br>著作权归作者所有，转载请联系作者获得授权。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"SVO-简介\"><a href=\"#SVO-简介\" class=\"headerlink\" title=\"SVO 简介\"></a>SVO 简介</h2><p><strong>SVO 全称 Semi-direct monocular Visual Odometry（半直接视觉里程计），是苏黎世大学机器人感知组的克里斯蒂安.弗斯特（<a href=\"http://www.cforster.ch/\" target=\"_blank\" rel=\"external\">Christian Forster</a>，主页：Christian Forster）等人，于2014年ICRA会议上发表的工作，随后在github开源：<a href=\"https://github.com/uzh-rpg/rpg_svo\" target=\"_blank\" rel=\"external\">uzh-rpg/rpg_svo</a>。2016年扩展了多相机和IMU之后，写成期刊论文，称为SVO 2.0，预定将在IEEE Trans. on Robotics上发表（视频见[2]）。SVO 2.0目前未开源（个人认为以后也不会开)</strong>","more":"</p>\n<p>SVO主要工作由弗斯特完成，此外他也在乔治亚理工的gtsam组呆过一段时间，参与了gtsam中IMU部分，文章发表在RSS 2015上，亦是VIO当中的著名工作[3]。此文章后续亦有期刊版本，预计也在TRO上发表。不过会议论文中公式推导有误，而且弗斯特本人似乎只参与了实现部分，没怎么管公式推导……当然这些都是八卦，不谈了。与SVO相关工作是同一组的REMODE[4]，实现了在SVO定位基础上的单目稠密建图（需要GPU），由SVO的二作马蒂亚（Matia Pizzoli）完成。</p>\n<p>SVO，虽然按照作者的理解，称为“半直接法”，然而按照我个人的理解，称为“稀疏直接法”可能更好一些。众所周知，VO主要分为特征点法和直接法。而SVO的实现中，混合使用了特征点与直接法：它跟踪了一些关键点（角点，没有描述子，由FAST实现），然后像直接法那样，根据这些关键点周围的信息，估计相机运动以及它们的位置。这与特征点法（以ORB-SLAM为代表）那样，需要对每张图像提取特征点和描述子的实现，是有明显不同的。所以，作者称之为“半直接法”。</p>\n<p>不过，由于SVO跟踪的“关键点”，亦可以理解成“梯度明显的像素”。从这个角度来看，它和LSD-SLAM[5]更加相近。只是LSD-SLAM跟踪了所有梯度明显的像素，形成半稠密地图；而SVO只跟踪稀疏的关键点，所以不妨称之为“稀疏直接法”。这一点，和今年慕尼黑理工丹尼尔.克莱默（Daniel Cremers）组的雅各布.恩格尔（Jakob Engel）提出的DSO[6]是很相似的。（PS：ORB,LSD,SVO几个作者似乎都去了同一个公司啊……那我还搞啥……）</p>\n<p>在视觉里程计中，直接法最突出的优点是非常快（ORB作者劳尔曾认为特征点比较快，我觉得是不对的）。而在直接法中（包括稀疏的，半稠密的以及稠密的），使用稀疏的直接法，既不必费力去计算描述子，也不必处理像稠密和半稠密那么多的信息，能够达到极快的速度。因此，SVO即使在低端计算平台上也能达到实时性，而在PC平台上则可以达到100多帧每秒的速度。在作者后续工作SVO 2.0中，速度更达到了惊人的400帧每秒。这使得SVO非常适用于计算平台受限的场合，例如无人机、手持AR/VR设备的定位。无人机也是弗斯特等人最初开发SVO的目标应用平台。</p>\n<p>SVO另一特点是实现了一种特有的深度滤波器（Depth Filter）。这里一种基于均匀——高斯混合分布的深度滤波器，由弗吉亚兹于2011年提取并推导[7]。由于原理较为复杂，之后再详细解释。SVO将这种滤波器用于关键点的深度估计，并使用了逆深度作为参数化形式，使之能够更好地计算特征点位置。这里SVO在建图线程中的主要任务。</p>\n<p>开源版的SVO代码清晰易读，十分适合读者作为第一个SLAM实例进行分析。初学者可以从SVO或ORB开始读起，弗斯特写代码一直比较清楚。</p>\n<p><strong>整个过程分为两个大模块：追踪与建图（与PTAM类似）。</strong></p>\n<ul>\n<li>上半部分为追踪部分。主要任务是估计当前帧的位姿。又分为两步：<ul>\n<li>先把当前帧和上一个追踪的帧进行比对，获取粗略的位姿。</li>\n<li>然后根据粗略的位姿，将它与地图进行比对，得到精确的位姿并优化见到的地图点。随后判断此帧是否为关键帧。如果为关键帧就提取新的特征点，把这些点作为地图的种子点，放入优化线程。否则，不为关键帧的时候，就用此帧的信息更新地图中种子点的深度估计值。</li>\n</ul>\n</li>\n<li>下半部分为建图部分。主要任务是估计特征点的深度。因为单目SLAM中，刚提的特征点是没有深度的，所以必须用新来的帧的信息，去更新这些特征点的深度分布，也就是所谓的“深度滤波器”。当某个点的深度收敛时，用它生成新的地图点，放进地图中，再被追踪部分使用。</li>\n</ul>\n<p>整个SVO架构要比ORB简单一些（ORB有三个线程，且要处理关键帧的共视关系和回环检测），所以效率也要高一些。下面详细谈这两个模块的做法。</p>\n<h2 id=\"追踪（Tracking）部分\"><a href=\"#追踪（Tracking）部分\" class=\"headerlink\" title=\"追踪（Tracking）部分\"></a>追踪（Tracking）部分</h2><p>追踪部分理解难点主要有两个：</p>\n<p>-如何计算帧与帧之间位姿变换？<br>-如何计算帧与地图之间的位姿变换？</p>\n<p>下面分别来说这两点。</p>\n<ol>\n<li>Frame-to-Frame的位姿变换<br>追踪的第一步是将当前帧与上一个追踪成功的帧进行对比，粗略估计当前帧的位姿。该问题的基本形式为：已知上一帧对地图点的观测（包括2D投影位置和深度），以及当前帧的图像，如何计算当前帧的位姿？用数学语言说，已经k-1帧的位姿T_{k-1}，并且知道它的观测量u<em>i, i=1, \\ldots, N时，求解T</em>{k-1,k}。</li>\n</ol>\n<p>在SVO里，该问题被称为 Model-based Image Alignment （带有相机模型的图像配准），实际上就是我们平时说的稀疏直接法。直接法的原理在我的博客中给出过比较细的推导：<a href=\"http://www.cnblogs.com/gaoxiang12/p/5689927.html\" target=\"_blank\" rel=\"external\">直接法 – 半闲居士 – 博客园</a> ，此外我也讲过一次讲座：<a href=\"http://www.bilibili.com/video/av6299156/\" target=\"_blank\" rel=\"external\">直接法的原理与实现_高翔<em>bilibili</em>演讲•公开课_科技<em>bilibili</em>哔哩哔哩弹幕视频网</a> 。本质上，它通过最小化前一帧和当前帧之间的光度误差来求得当前帧的（粗略）位姿：</p>\n<p>要理解它，你需要非线性优化的基本知识。同时，为了求目标函数相对于位姿的导数，你需要学习一些李代数的知识。这在我的博客和讲座中均有比较详细的探讨：</p>\n<ul>\n<li><a href=\"http://www.bilibili.com/video/av7705856/\" target=\"_blank\" rel=\"external\">视觉slam第4章<em>演讲•公开课</em>科技<em>bilibili</em>哔哩哔哩弹幕视频网</a></li>\n<li><a href=\"https://pan.baidu.com/s/1c2qPdle\" target=\"_blank\" rel=\"external\">非线性优化与g2o录像：http://pan.baidu.com/s/1c2qPdle</a></li>\n</ul>\n<p>由于不想在知乎打公式，所以请读者去看视频和讲座以了解其中原理（因为都已经讲过一遍了）。实现当中，SVO自己实现了高斯——牛顿法的迭代下降，并且比较取巧地使用了一种反向的求导方式：即雅可比在k-1帧图像上进行估计，而不在k帧上估计。这样做法的好处是在迭代过程中只需计算一次雅可比，其余部分只需更新残差即可（即G-N等式右侧的-J^Te）。这能够节省一定程度的计算量。另一个好处是，我们能够保证k-1帧的像素具有梯度，但没法保证在k帧上的投影也具有梯度，所以这样做至少能保证像素点的梯度是明显的。</p>\n<p>实现当中另一个需要注意的地方是金字塔的处理。这一步估计是从金字塔的顶层开始，把上一层的结果作为下一层估计的初始值，最后迭代到底层的。顶层的分辨率最小，所以这是一个由粗到精的过程（Coarse-to-Fine），使得在运动较大时也能有较好的结果。</p>\n<p>值得一提的是，完全可以使用优化库，例如g2o或ceres来实现所有的步骤。在优化库中，可以选用更多的优化方式，而且L-M或Dogleg的结果也会比G-N更有保证。我自己就用两个库各实现过一遍。</p>\n<ol>\n<li>Frame-to-Map<br>在1求解之后，我们得到了当前帧位姿的粗略估计。因为它是基于上一帧的结果来计算的，所以如果把它当作真实位姿估计的话，将有较大的累积误差。因此，需要进一步和地图之间进行特征点比对，来对当前帧位姿进一步优化。主要步骤如下：</li>\n</ol>\n<ul>\n<li>遍历地图中的所有点，计算在当前帧的投影位置。由于当前帧有粗略的位姿估计，这个投影位置应该与真实位置有少量误差（2~3个像素）。</li>\n<li>对每个成功投影的地图点，比较这些点的初始观测图像与当前帧的图像。通过计算光度的误差，求取更精准的投影位置。这步类似于光流，在SVO中称为Refinement。</li>\n<li>根据更精确的投影位置，进行位姿与地图点的优化。这一步类似于Bundle Adjustment，但SVO实现中，是把Pose和Point两个问题拆开优化的，速度更快。</li>\n<li>判断是否生成关键帧，处理关键帧的生成。</li>\n</ul>\n<p>这里理解的难点是，地图点初次被观测到的图像与当前帧的图像进行比对时，不能直接对两个图像块求差，而需要计算一个仿射变换（Affine Warp）。这是因为初次观测和当前帧的位移较远，并且可能存在旋转，所以不能单纯地假设图像块不变。仿射变换的计算方式在PTAM论文的5.3节有介绍，似乎是一种比较标准的处理方式。（其实SVO的追踪部分和PTAM整个儿都挺像。）</p>\n<p>实现当中可能还需要注意一些细节。例如有些地方使用了网格，以保证观测点的均匀分布。还有Affine Warp当中需要注意特征点所在的金字塔层数，等等。</p>\n<p>此后的Bundle Adjustment部分和传统的区别不大，除了把pose和point分开计算之外。关键帧判断方面，SVO是比较薄弱的（考虑的东西太少），和ORB相比差了不少。</p>\n<h2 id=\"Mapping部分\"><a href=\"#Mapping部分\" class=\"headerlink\" title=\"Mapping部分\"></a>Mapping部分</h2><p>Mapping部分主要是计算特征点的深度。如前所言，单目VO中，刚刚从图像中提取的热乎的关键点是没有深度的，需要等相机位移之后再通过三角化，再估计这些点的深度。这些尚未具备有效深度信息的点，不妨称之为种子点（或候选点）。然而，三角化的成功与否（以及精度），取决于相机之间的平移量和视线的夹角，所以我们通常要维护种子点的深度分布，而不是单纯的一个深度值</p>\n<p>牵涉到概率分布的，往往都是理论一大堆屁话，实际可以操作的只有高斯分布一种——高斯只要在计算机里存均值和协方差即可。在逆深度[8]流行起来之后，用逆深度的高斯分布成了SLAM中的标配。然而SVO却使用了一种高斯——均匀混合分布的逆深度（由四个参数描述），推导并实现了它的更新方式，称为Depth Filter。它的工作逻辑大概是这样的：</p>\n<ul>\n<li>如果进来一个关键帧，就提取关键帧上的新特征点，作为种子点放进一个种子队列中。</li>\n<li>如果进来一个普通帧，就用普通帧的信息，更新所有种子点的概率分布。如果某个种子点的深度分布已经收敛，就把它放到地图中，供追踪线程使用。</li>\n</ul>\n<p>当然实现当中还有一些细节，比如删掉时间久远的种子点、删掉很少被看到的种子点等等。</p>\n<p>要理解Depth Filter，请搞清楚这两件事：</p>\n<pre><code>1. 基于高斯——均匀的滤波器，在理论上的推导是怎么样的？\n2. Depth Filter又是如何利用普通帧的信息去更新种子点的？\n</code></pre><p>第1个问题，请参照论文[4],[7],以及[7]的补充材料，以及补充材料的补充材料。相信研究SVO的人应该都推导过，并不很难，静下心来推一遍即可，我当时也就一块小白板就推完了。在SLAM群的群文件里有一个depth filter.pdf，也给出了推导过程：<br>或者请看<a href=\"http://www.cnblogs.com/luyb/p/5773691.html\" target=\"_blank\" rel=\"external\">SVO原理解析 – 路游侠 – 博客园</a> 。我觉得应该用不着把公式在知乎上再敲一遍……</p>\n<p>第2个问题，你需要搞明白极线搜索这件事。由于种子点的深度不确定，它在别的帧里看起来就在一条直线（极线）上：<br>于是你从这条极线的一个端点走到另一个端点，把每处的图像块都和参考的去比较，就可以（可能）找到正确的匹配了。哦别忘了要Affine Warp一下……找到之后，让depth filter更新其深度分布即可。当然如果位移太小或视线平行性太高，让深度变得更加不确定也是有可能的。在理想情况下，你可以期待一个地图点经过不断观测之后收敛的过程。</p>\n<h2 id=\"评述\"><a href=\"#评述\" class=\"headerlink\" title=\"评述\"></a>评述</h2><p>以上就是SVO的基本工作原理了。那么，这样一套系统实际工作起来效果如何呢？相比于其他几个开源方案有何优劣呢？</p>\n<p>首先要澄清一点的是：开源版本的SVO，是一个比较挫的版本。相比于LSD或ORB，我还很少看到有人能一次性把SVO跑通的。但是从论文上看，开源版本并不能代表SVO的真实水平。所以应该是心机弗斯特开源了一个只有部分代码的，不怎么好用的版本，仅供学习研究使用。相比之下，DSO，LSD，ORB至少能够在自己数据集上顺利运行，而ORB、LSD还能在大部分自定义的相机上运行，这点开源版本的SVO是做不到的。</p>\n<p>那么，抛开开源实现，从理论和框架上来说，SVO有何优劣呢？</p>\n<p>优点：</p>\n<ul>\n<li>着实非常快，不愧为稀疏直接法；</li>\n<li>关键点分布比较均匀；</li>\n</ul>\n<p>缺点：（不是我嫌弃它，确实有一堆可以吐槽的地方）</p>\n<ol>\n<li>首先这货是VO，不是SLAM，没有闭环。这意味着丢失后没法重定位——丢了基本就挂了。</li>\n<li>追踪部分：SVO首先将当前帧与上一个追踪的帧比较，以求得粗略的位姿估计。这里存在一个问题：这必须要求上一个帧是足够准确的！那么问题就来了：怎么知道上一个帧是准的呢？开源SVO里甚少考虑出错的情况。如果上一个帧由于遮挡、模糊等原因丢失，那么当前帧也就会得到一个错误的结果，导致和地图比对不上。又由于这货是没法重定位的，所以就。。。挂了呗。。。</li>\n<li><p>还是追踪部分。既然是直接法，SVO就有直接法的所有缺点。后面那位同学来背一遍直接法缺点？</p>\n<ul>\n<li>怕模糊（需要全局曝光相机）</li>\n<li>怕大运动（图像非凸性）</li>\n<li>怕光照变化（灰度不变假设）</li>\n</ul>\n</li>\n<li><p>地图部分：</p>\n<ul>\n<li>Depth Filter收敛较慢，结果比较严重地依赖于准确的位姿估计。如果统计收敛的种子点的比例，会发现并不高，很多计算浪费在不收敛的点上。</li>\n<li>相比于纯高斯的逆深度，SVO的Depth Filter主要特点是能够通过Beta分布中的两个参数a,b来判断一个种子点是否为outlier。然而在特征点法中我们也能够通过描述来判断outlier，所以并不具有明显优势。</li>\n</ul>\n</li>\n</ol>\n<h2 id=\"小结\"><a href=\"#小结\" class=\"headerlink\" title=\"小结\"></a>小结</h2><ol>\n<li>SVO是基于稀疏直接法的视觉里程计，速度非常快。</li>\n<li>代码清晰易读，研究SVO会有不少启发。</li>\n<li>但是开源实现存在诸多缺点，不实用。论文中效果应该不是这个开源代码能够实现的。</li>\n</ol>\n<p>参考文献<br>[1] Foster et al., SVO: Fast semi-direct monocular visual odometry, ICRA 2014.<br>[2] SVO 2.0<br>[3] Foster et al., IMU preintegration on manifold for efficient visual-inertial maximum-a-posteriori estimation, RSS 2015.<br>[4] Pizzoli, Matia and Forster, Christian and Scaramuzza, Davide, REMODE: Probabilistic, monocular dense reconstruction in real time, ICRA 2014.<br>[5] Engel, Jakob and Schops, Thomas and Cremers, Daniel, LSD-SLAM: Large-scale direct monocular SLAM, ECCV, 2014.<br>[6] Engel, Jakob and Koltun, Vladlen and Cremers, Daniel, Direct sparse odometry, 2016.<br>[7] George Vogiatzis and Carlos Hernández, Video-based, real-time multi-view stereo, Image and Vision Computing, 2011.<br>[8] Civera, Javier and Davison, Andrew J and Montiel, JM Martinez, Inverse depth parametrization for monocular SLAM, IEEE transactions on robotics, 2008.</p>\n<p>附：SVO相关中文博客、材料<br><a href=\"http://fengbing.net/\" target=\"_blank\" rel=\"external\">冯兵的博客|内外兼修 &lt;一步步完善视觉里程计&gt;是对SVO比较完整的介绍。</a><br><a href=\"http://www.voidcn.com/blog/wxzxur\" target=\"_blank\" rel=\"external\">白巧克力亦唯心的博客 – 程序园</a> 贺一家的博客，有几篇关于SVO的介绍。<br><a href=\"http://www.cnblogs.com/luyb/\" target=\"_blank\" rel=\"external\">路游侠 – 博客园</a> 这个是谁。。。知道的回复我一下。。。<br><a href=\"http://www.bilibili.com/video/av5934066/\" target=\"_blank\" rel=\"external\">ORB-LSD-SVO比较-刘浩敏<em>bilibili</em>演讲•公开课_科技<em>bilibili</em>哔哩哔哩弹幕视频网</a> 浩敏师兄讲的一次ppt</p>\n<p>作者：半闲居士<br>链接：<a href=\"https://www.zhihu.com/question/39904950/answer/138644975\" target=\"_blank\" rel=\"external\">https://www.zhihu.com/question/39904950/answer/138644975</a><br>来源：知乎<br>著作权归作者所有，转载请联系作者获得授权。</p>"},{"title":"DDNS","date":"2017-08-29T11:54:42.000Z","_content":"\nBackground\n---\n**项目所用的服务器是基础电信线路，没有申请固定ip，需要使用ddns实时更新服务器ipv4地址。**  <!--more-->\n\nFeatures\n---\n&emsp;&emsp;最新版本的花生壳已不再支持动态域名（ddns），仅支持内网穿透。<br>&emsp;&emsp;而花生壳之前发布的软件phddns2.0仍支持ddns，在[http://hsk.oray.com/download/](http://hsk.oray.com/download/)下载经典版即可。 <br>\n&emsp;&emsp;在ubuntu下，使用dpkg安装***(sudo dpkg -i  [软件包名])***后，直接运行phddns即可进行交互式配置。服务器配置和网卡配置保持默认即可，花生壳官网的账号就是登陆所需的用户名密码。交互式配置完成后ddns服务就开始运行。***务必记住配置文件所在位置，以便之后修改，这个位置可以在先前的交互式配置中设置***。服务正常运行后，即可把它加入系统启动项中。<br>\n&emsp;&emsp;在ubuntu下，自订服务脚本需要自定init info，否则会报错。<br>\n&emsp;&emsp;以ubuntu16.04LTS为例，在/etc/init.d中新建一个shell脚本<br>\n```\nsudo vi phddns-autorun.sh\n```\n按i进入插入模式，\n然后录入以下文本，#!/bin/sh以下是init info部分\n```\n#!/bin/sh\n### BEGIN INIT INFO\n# Provides:          phddns_oray_ddns\n# Required-Start:    $local_fs $network\n# Required-Stop:     $local_fs\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: phddnsd\n# Description:       phddns daemon\n### END INIT INFO\nphddns -d\nexit 0\n```\nphddns -d 以守护程序方式运行，在前面的交互式配置之后，该方式会默认使用先前设置好的配置文件。<br>\n&emsp;&emsp;录入完成后按ESC然后输入:wq!保存缓冲区并退出。然后把该脚本设置为默认情况下的开机启动项。<br>\n```\nsudo update-rc.d phddns-autorun.sh defaults\n```\n<br>\n&emsp;&emsp;设置成功后，为该服务器设置的动态域名可到[花生壳控制台](https://b.oray.com/)登录查看，默认情况下所有的壳域名均可作为动态域名使用。<br>\n&emsp;&emsp;花生壳官网同时提供了可免费申请的ddns服务SDK，可用于服务器集群的ddns配置。\n\n\n\n","source":"_posts/DDNS.md","raw":"---\ntitle: DDNS\ndate: 2017-08-29 19:54:42\ntags:\n---\n\nBackground\n---\n**项目所用的服务器是基础电信线路，没有申请固定ip，需要使用ddns实时更新服务器ipv4地址。**  <!--more-->\n\nFeatures\n---\n&emsp;&emsp;最新版本的花生壳已不再支持动态域名（ddns），仅支持内网穿透。<br>&emsp;&emsp;而花生壳之前发布的软件phddns2.0仍支持ddns，在[http://hsk.oray.com/download/](http://hsk.oray.com/download/)下载经典版即可。 <br>\n&emsp;&emsp;在ubuntu下，使用dpkg安装***(sudo dpkg -i  [软件包名])***后，直接运行phddns即可进行交互式配置。服务器配置和网卡配置保持默认即可，花生壳官网的账号就是登陆所需的用户名密码。交互式配置完成后ddns服务就开始运行。***务必记住配置文件所在位置，以便之后修改，这个位置可以在先前的交互式配置中设置***。服务正常运行后，即可把它加入系统启动项中。<br>\n&emsp;&emsp;在ubuntu下，自订服务脚本需要自定init info，否则会报错。<br>\n&emsp;&emsp;以ubuntu16.04LTS为例，在/etc/init.d中新建一个shell脚本<br>\n```\nsudo vi phddns-autorun.sh\n```\n按i进入插入模式，\n然后录入以下文本，#!/bin/sh以下是init info部分\n```\n#!/bin/sh\n### BEGIN INIT INFO\n# Provides:          phddns_oray_ddns\n# Required-Start:    $local_fs $network\n# Required-Stop:     $local_fs\n# Default-Start:     2 3 4 5\n# Default-Stop:      0 1 6\n# Short-Description: phddnsd\n# Description:       phddns daemon\n### END INIT INFO\nphddns -d\nexit 0\n```\nphddns -d 以守护程序方式运行，在前面的交互式配置之后，该方式会默认使用先前设置好的配置文件。<br>\n&emsp;&emsp;录入完成后按ESC然后输入:wq!保存缓冲区并退出。然后把该脚本设置为默认情况下的开机启动项。<br>\n```\nsudo update-rc.d phddns-autorun.sh defaults\n```\n<br>\n&emsp;&emsp;设置成功后，为该服务器设置的动态域名可到[花生壳控制台](https://b.oray.com/)登录查看，默认情况下所有的壳域名均可作为动态域名使用。<br>\n&emsp;&emsp;花生壳官网同时提供了可免费申请的ddns服务SDK，可用于服务器集群的ddns配置。\n\n\n\n","slug":"DDNS","published":1,"updated":"2017-08-29T12:30:17.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyo90003u2vxv43syrlh","content":"<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><p><strong>项目所用的服务器是基础电信线路，没有申请固定ip，需要使用ddns实时更新服务器ipv4地址。</strong>  <a id=\"more\"></a></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>&emsp;&emsp;最新版本的花生壳已不再支持动态域名（ddns），仅支持内网穿透。<br>&emsp;&emsp;而花生壳之前发布的软件phddns2.0仍支持ddns，在<a href=\"http://hsk.oray.com/download/\" target=\"_blank\" rel=\"external\">http://hsk.oray.com/download/</a>下载经典版即可。 <br><br>&emsp;&emsp;在ubuntu下，使用dpkg安装<strong><em>(sudo dpkg -i  [软件包名])</em></strong>后，直接运行phddns即可进行交互式配置。服务器配置和网卡配置保持默认即可，花生壳官网的账号就是登陆所需的用户名密码。交互式配置完成后ddns服务就开始运行。<strong><em>务必记住配置文件所在位置，以便之后修改，这个位置可以在先前的交互式配置中设置</em></strong>。服务正常运行后，即可把它加入系统启动项中。<br><br>&emsp;&emsp;在ubuntu下，自订服务脚本需要自定init info，否则会报错。<br><br>&emsp;&emsp;以ubuntu16.04LTS为例，在/etc/init.d中新建一个shell脚本<br><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo vi phddns-autorun.sh</div></pre></td></tr></table></figure></p>\n<p>按i进入插入模式，<br>然后录入以下文本，#!/bin/sh以下是init info部分<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/bin/sh</div><div class=\"line\">### BEGIN INIT INFO</div><div class=\"line\"># Provides:          phddns_oray_ddns</div><div class=\"line\"># Required-Start:    $local_fs $network</div><div class=\"line\"># Required-Stop:     $local_fs</div><div class=\"line\"># Default-Start:     2 3 4 5</div><div class=\"line\"># Default-Stop:      0 1 6</div><div class=\"line\"># Short-Description: phddnsd</div><div class=\"line\"># Description:       phddns daemon</div><div class=\"line\">### END INIT INFO</div><div class=\"line\">phddns -d</div><div class=\"line\">exit 0</div></pre></td></tr></table></figure></p>\n<p>phddns -d 以守护程序方式运行，在前面的交互式配置之后，该方式会默认使用先前设置好的配置文件。<br><br>&emsp;&emsp;录入完成后按ESC然后输入:wq!保存缓冲区并退出。然后把该脚本设置为默认情况下的开机启动项。<br><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo update-rc.d phddns-autorun.sh defaults</div></pre></td></tr></table></figure></p>\n<p><br><br>&emsp;&emsp;设置成功后，为该服务器设置的动态域名可到<a href=\"https://b.oray.com/\" target=\"_blank\" rel=\"external\">花生壳控制台</a>登录查看，默认情况下所有的壳域名均可作为动态域名使用。<br><br>&emsp;&emsp;花生壳官网同时提供了可免费申请的ddns服务SDK，可用于服务器集群的ddns配置。</p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Background\"><a href=\"#Background\" class=\"headerlink\" title=\"Background\"></a>Background</h2><p><strong>项目所用的服务器是基础电信线路，没有申请固定ip，需要使用ddns实时更新服务器ipv4地址。</strong>","more":"</p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>&emsp;&emsp;最新版本的花生壳已不再支持动态域名（ddns），仅支持内网穿透。<br>&emsp;&emsp;而花生壳之前发布的软件phddns2.0仍支持ddns，在<a href=\"http://hsk.oray.com/download/\" target=\"_blank\" rel=\"external\">http://hsk.oray.com/download/</a>下载经典版即可。 <br><br>&emsp;&emsp;在ubuntu下，使用dpkg安装<strong><em>(sudo dpkg -i  [软件包名])</em></strong>后，直接运行phddns即可进行交互式配置。服务器配置和网卡配置保持默认即可，花生壳官网的账号就是登陆所需的用户名密码。交互式配置完成后ddns服务就开始运行。<strong><em>务必记住配置文件所在位置，以便之后修改，这个位置可以在先前的交互式配置中设置</em></strong>。服务正常运行后，即可把它加入系统启动项中。<br><br>&emsp;&emsp;在ubuntu下，自订服务脚本需要自定init info，否则会报错。<br><br>&emsp;&emsp;以ubuntu16.04LTS为例，在/etc/init.d中新建一个shell脚本<br><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo vi phddns-autorun.sh</div></pre></td></tr></table></figure></p>\n<p>按i进入插入模式，<br>然后录入以下文本，#!/bin/sh以下是init info部分<br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div><div class=\"line\">2</div><div class=\"line\">3</div><div class=\"line\">4</div><div class=\"line\">5</div><div class=\"line\">6</div><div class=\"line\">7</div><div class=\"line\">8</div><div class=\"line\">9</div><div class=\"line\">10</div><div class=\"line\">11</div><div class=\"line\">12</div></pre></td><td class=\"code\"><pre><div class=\"line\">#!/bin/sh</div><div class=\"line\">### BEGIN INIT INFO</div><div class=\"line\"># Provides:          phddns_oray_ddns</div><div class=\"line\"># Required-Start:    $local_fs $network</div><div class=\"line\"># Required-Stop:     $local_fs</div><div class=\"line\"># Default-Start:     2 3 4 5</div><div class=\"line\"># Default-Stop:      0 1 6</div><div class=\"line\"># Short-Description: phddnsd</div><div class=\"line\"># Description:       phddns daemon</div><div class=\"line\">### END INIT INFO</div><div class=\"line\">phddns -d</div><div class=\"line\">exit 0</div></pre></td></tr></table></figure></p>\n<p>phddns -d 以守护程序方式运行，在前面的交互式配置之后，该方式会默认使用先前设置好的配置文件。<br><br>&emsp;&emsp;录入完成后按ESC然后输入:wq!保存缓冲区并退出。然后把该脚本设置为默认情况下的开机启动项。<br><br><figure class=\"highlight plain\"><table><tr><td class=\"gutter\"><pre><div class=\"line\">1</div></pre></td><td class=\"code\"><pre><div class=\"line\">sudo update-rc.d phddns-autorun.sh defaults</div></pre></td></tr></table></figure></p>\n<p><br><br>&emsp;&emsp;设置成功后，为该服务器设置的动态域名可到<a href=\"https://b.oray.com/\" target=\"_blank\" rel=\"external\">花生壳控制台</a>登录查看，默认情况下所有的壳域名均可作为动态域名使用。<br><br>&emsp;&emsp;花生壳官网同时提供了可免费申请的ddns服务SDK，可用于服务器集群的ddns配置。</p>"},{"title":"Principle of real time video transmission","date":"2016-01-10T11:08:39.000Z","_content":"\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**","source":"_posts/Principle-of-real-time-video-transmission.md","raw":"title: Principle of real time video transmission\ndate: 2016-01-10 19:08:39\ntags:\n---\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**","slug":"Principle-of-real-time-video-transmission","published":1,"updated":"2016-01-10T11:45:57.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyof0005u2vxc13uhiow","content":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong><a id=\"more\"></a></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong>","more":"</p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>"},{"title":"What is robot?","date":"2016-01-10T10:53:36.000Z","_content":"\nIn our dream, there always some site where the tree is green, the sky is blue, the stream is running. However, I can't see it again.<!--more-->\n\nindustry\n---\n\nFllowly the time run, our country is changing more beautiful, the sky is coming soon. our rocket is going to space!!!!\n","source":"_posts/What-is-robot.md","raw":"title: What is robot?\ndate: 2016-01-10 18:53:36\ntags: \n- hello\n- robot\n- helloworld\ncategories: \n- robot\n- Java\n- Android\n\n---\n\nIn our dream, there always some site where the tree is green, the sky is blue, the stream is running. However, I can't see it again.<!--more-->\n\nindustry\n---\n\nFllowly the time run, our country is changing more beautiful, the sky is coming soon. our rocket is going to space!!!!\n","slug":"What-is-robot","published":1,"updated":"2016-01-11T10:29:28.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyor0007u2vxxetzybvr","content":"<p>In our dream, there always some site where the tree is green, the sky is blue, the stream is running. However, I can’t see it again.<a id=\"more\"></a></p>\n<h2 id=\"industry\"><a href=\"#industry\" class=\"headerlink\" title=\"industry\"></a>industry</h2><p>Fllowly the time run, our country is changing more beautiful, the sky is coming soon. our rocket is going to space!!!!</p>\n","site":{"data":{}},"excerpt":"<p>In our dream, there always some site where the tree is green, the sky is blue, the stream is running. However, I can’t see it again.","more":"</p>\n<h2 id=\"industry\"><a href=\"#industry\" class=\"headerlink\" title=\"industry\"></a>industry</h2><p>Fllowly the time run, our country is changing more beautiful, the sky is coming soon. our rocket is going to space!!!!</p>"},{"title":"land robot already open","date":"2016-01-10T10:59:43.000Z","_content":"\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**\n","source":"_posts/land-robot-already-open.md","raw":"title: land robot already open\ndate: 2016-01-10 18:59:43\ntags:\n---\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**\n","slug":"land-robot-already-open","published":1,"updated":"2016-01-10T11:49:04.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyoz0009u2vxt4n6bkzr","content":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong><a id=\"more\"></a></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong>","more":"</p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>"},{"title":"robot control principle","date":"2016-01-10T11:07:21.000Z","_content":"\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n![hei](/img/icon.png)\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**","source":"_posts/robot-control-principle.md","raw":"title: robot control principle\ndate: 2016-01-10 19:07:21\ntags: \n- goal\n- directions\n- donation\ncategories: algorithm\n---\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n![hei](/img/icon.png)\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**","slug":"robot-control-principle","published":1,"updated":"2016-01-11T02:29:53.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyp3000cu2vx1h0uukhv","content":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong><a id=\"more\"></a></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous<br><img src=\"/img/icon.png\" alt=\"hei\"></p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong>","more":"</p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous<br><img src=\"/img/icon.png\" alt=\"hei\"></p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>"},{"title":"the big data is coming!","date":"2016-01-10T10:58:30.000Z","_content":"\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**","source":"_posts/the-big-data-is-coming.md","raw":"title: 'the big data is coming!'\ndate: 2016-01-10 18:58:30\ntags:\n- bigdata\n- SAE\n- goal\n---\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**","slug":"the-big-data-is-coming","published":1,"updated":"2016-01-11T10:26:22.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyp5000du2vxmc1tan86","content":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong><a id=\"more\"></a></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong>","more":"</p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>"},{"title":"X-Space Blog","date":"2016-01-10T05:10:18.000Z","_content":"\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**\n","source":"_posts/this-is-a-first-blog.md","raw":"title: X-Space Blog\ndate: 2016-01-10 13:10:18\ntags:\n---\n\nOur team goal\n---\n**Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!**<!--more-->\n\nFeatures\n---\nIn the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.\n\n* Real time audio and video\n* Real time motion control\n* Remote communication in the Internet\n* Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)\n\nOur robots control methods include: autonomous and serni autonomous\n\nRelated directions\n---\nMainly related to the technical direction:\n\n* PCB plate making technology\n* Single chip microcomputer control technology\n* Embedded operating system\n* Android Application development\n* Sensor technology\n* Web Development technology\n\nRobot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.\n\nDonation\n---\nThis project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.\n\n**Alipay.com account: 370045744@qq.com**\n","slug":"this-is-a-first-blog","published":1,"updated":"2016-01-10T11:45:09.000Z","comments":1,"layout":"post","photos":[],"link":"","_id":"cj7fpvyp8000eu2vxhqbz7y2h","content":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong><a id=\"more\"></a></p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>\n","site":{"data":{}},"excerpt":"<h2 id=\"Our-team-goal\"><a href=\"#Our-team-goal\" class=\"headerlink\" title=\"Our team goal\"></a>Our team goal</h2><p><strong>Our team is coming from China, named of X-Space. We are young, and we are naive, building an open source platform for the robot to provide solutions for the foundation of the robot, to help more people to better understand the industry, is our goal!!!!!</strong>","more":"</p>\n<h2 id=\"Features\"><a href=\"#Features\" class=\"headerlink\" title=\"Features\"></a>Features</h2><p>In the future, we will provide open source solutions for land robots, air robots, underwater robots, etc.</p>\n<ul>\n<li>Real time audio and video</li>\n<li>Real time motion control</li>\n<li>Remote communication in the Internet</li>\n<li>Robot arm and a variety of sensor peripherals(including infrared, ultrasound, etc)</li>\n</ul>\n<p>Our robots control methods include: autonomous and serni autonomous</p>\n<h2 id=\"Related-directions\"><a href=\"#Related-directions\" class=\"headerlink\" title=\"Related directions\"></a>Related directions</h2><p>Mainly related to the technical direction:</p>\n<ul>\n<li>PCB plate making technology</li>\n<li>Single chip microcomputer control technology</li>\n<li>Embedded operating system</li>\n<li>Android Application development</li>\n<li>Sensor technology</li>\n<li>Web Development technology</li>\n</ul>\n<p>Robot is a multidisciplinary blend of subject direction, need more people to participate in, with our hands together to create a better future.</p>\n<h2 id=\"Donation\"><a href=\"#Donation\" class=\"headerlink\" title=\"Donation\"></a>Donation</h2><p>This project is completely independent by our team to provide finance to support, currently, do not accept financing. If you think our team is contribute for you! And to thank us, you can give Alipay transfer donations.</p>\n<p><strong>Alipay.com account: 370045744@qq.com</strong></p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cj7fpvyp3000cu2vx1h0uukhv","category_id":"cj7fpvyp9000fu2vx19q1hcpk","_id":"cj7fpvypd000ju2vxflu0dqyt"},{"post_id":"cj7fpvyor0007u2vxxetzybvr","category_id":"cj7fpvyp0000au2vxzwdan7rh","_id":"cj7fpvypk000ru2vxvnqas2n8"},{"post_id":"cj7fpvyor0007u2vxxetzybvr","category_id":"cj7fpvypb000hu2vxbvvjo15n","_id":"cj7fpvypk000su2vx064774jh"},{"post_id":"cj7fpvyor0007u2vxxetzybvr","category_id":"cj7fpvypj000mu2vxcvzooxx7","_id":"cj7fpvypl000uu2vxm6v6g6vw"}],"PostTag":[{"post_id":"cj7fpvyor0007u2vxxetzybvr","tag_id":"cj7fpvyp2000bu2vx2g45dvf7","_id":"cj7fpvypj000lu2vx9eun7rji"},{"post_id":"cj7fpvyor0007u2vxxetzybvr","tag_id":"cj7fpvyp9000gu2vxqhagc0eu","_id":"cj7fpvypj000nu2vxi4lt8hkg"},{"post_id":"cj7fpvyor0007u2vxxetzybvr","tag_id":"cj7fpvypb000iu2vx9xazr9zm","_id":"cj7fpvypj000pu2vxdugvj43q"},{"post_id":"cj7fpvyp3000cu2vx1h0uukhv","tag_id":"cj7fpvypi000ku2vxo4vfhsqy","_id":"cj7fpvypm000vu2vxaxezcp37"},{"post_id":"cj7fpvyp3000cu2vx1h0uukhv","tag_id":"cj7fpvypj000ou2vxzecc7l8i","_id":"cj7fpvypm000wu2vxcinan2ze"},{"post_id":"cj7fpvyp3000cu2vx1h0uukhv","tag_id":"cj7fpvypk000qu2vx7wkmn6tn","_id":"cj7fpvypn000yu2vxl4huwkti"},{"post_id":"cj7fpvyp5000du2vxmc1tan86","tag_id":"cj7fpvypk000tu2vx949y41jo","_id":"cj7fpvypr0010u2vxn6t11usd"},{"post_id":"cj7fpvyp5000du2vxmc1tan86","tag_id":"cj7fpvypm000xu2vxlnkbhnr5","_id":"cj7fpvypr0011u2vxsk8e3d31"},{"post_id":"cj7fpvyp5000du2vxmc1tan86","tag_id":"cj7fpvypi000ku2vxo4vfhsqy","_id":"cj7fpvypr0012u2vx5udq6gon"}],"Tag":[{"name":"hello","_id":"cj7fpvyp2000bu2vx2g45dvf7"},{"name":"robot","_id":"cj7fpvyp9000gu2vxqhagc0eu"},{"name":"helloworld","_id":"cj7fpvypb000iu2vx9xazr9zm"},{"name":"goal","_id":"cj7fpvypi000ku2vxo4vfhsqy"},{"name":"directions","_id":"cj7fpvypj000ou2vxzecc7l8i"},{"name":"donation","_id":"cj7fpvypk000qu2vx7wkmn6tn"},{"name":"bigdata","_id":"cj7fpvypk000tu2vx949y41jo"},{"name":"SAE","_id":"cj7fpvypm000xu2vxlnkbhnr5"}]}}